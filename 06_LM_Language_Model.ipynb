{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liadmagen/NLP-Course/blob/master/06_LM_Language_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHpU4n2ZAd2p"
      },
      "source": [
        "# Language Model Example\n",
        "\n",
        "In this notebook, you'll train a n-gram Language model yourself"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYs_Dhab9syd"
      },
      "source": [
        "This example is based on:\n",
        "\n",
        "https://nlpforhackers.io/language-models/\n",
        "\n",
        "For a (very!) detailed information about this topic, please refer to:\n",
        "https://web.stanford.edu/~jurafsky/slp3/3.pdf "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxk8CHMfAr1H"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piIx18HE_dlq",
        "outputId": "b51f4aac-704d-4bba-ae97-267998149dcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: This step is only necessary when running this script in Google colab\n",
        "# due to a limitation of Google Colab to unzip the reuters corpora. \n",
        "# It is not needed when using nltk locally on your computer.\n",
        "\n",
        "from zipfile import ZipFile\n",
        "\n",
        "file_loc = '/root/nltk_data/corpora/reuters.zip'\n",
        "\n",
        "with ZipFile(file_loc, 'r') as z:\n",
        "  z.extractall('/root/nltk_data/corpora/')"
      ],
      "metadata": {
        "id": "lRJYAcADzmQa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjCnv0889m7r"
      },
      "source": [
        "import random\n",
        "from functools import reduce\n",
        "\n",
        "from operator import mul\n",
        "from collections import Counter\n",
        "\n",
        "from nltk.corpus import reuters\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        " "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj4-vpdmAuLu"
      },
      "source": [
        "# Bag-of-words & Frequency calculation\n",
        "\n",
        "We start by constructing a Bag-of-words.\n",
        "\n",
        "Remind yourself that a Language Model is a calculation of the frequencies and the probabilities of words, based on their observed occurrences.\n",
        "\n",
        "We calculate the word frequencies, by simply counting the word occurrences."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reuters.words()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjQ14BTLyfVt",
        "outputId": "9b367017-40e8-4954-af2e-8fab143469a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI732WCZ9wsv",
        "outputId": "f2348187-bfd8-458d-bb57-16bf8f6cac1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "counts = Counter(reuters.words())\n",
        "total_count = len(reuters.words())\n",
        "\n",
        "print(counts.most_common(n=20))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('.', 94687), (',', 72360), ('the', 58251), ('of', 35979), ('to', 34035), ('in', 26478), ('said', 25224), ('and', 25043), ('a', 23492), ('mln', 18037), ('vs', 14120), ('-', 13705), ('for', 12785), ('dlrs', 11730), (\"'\", 11272), ('The', 10968), ('000', 10277), ('1', 9977), ('s', 9298), ('pct', 9093)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnxBoLt6-Rsz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b770da58-b132-4744-83f1-f5881f401409"
      },
      "source": [
        "# Compute the frequencies\n",
        "for word in counts:\n",
        "    counts[word] /= float(total_count)\n",
        "\n",
        "# The frequencies should add up to 1\n",
        "print(sum(counts.values()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0000000000006808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzx-cyghBJzA"
      },
      "source": [
        "Let's create a random text passage, and calculate what is the probability that such a sentence is valid.\n",
        "\n",
        "Before continuing, and executing the next cells: \n",
        "\n",
        "**please pause and guess:**\n",
        "* what do you expect that the probability of a random 5 words passage would be? \n",
        "* How about a 100 random words?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4FefXrB9_YN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88b9694c-1808-4476-fdaf-e4b4409bc8f2"
      },
      "source": [
        " # Generate 100 words of language\n",
        "text = []\n",
        " \n",
        "for _ in range(100):\n",
        "    r = random.random()\n",
        "    accumulator = .0\n",
        " \n",
        "    for word, freq in counts.items():\n",
        "        accumulator += freq\n",
        " \n",
        "        if accumulator >= r:\n",
        "            text.append(word)\n",
        "            break\n",
        " \n",
        "print(' '.join(text))\n",
        " "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "said lt 44 , accord U may held said take the , . official - major to can 000 into at Commission ENERGY apparent previously imports Inc and as Tel \" from . to Below J 314 Antonson 00 for weekend earned Recent Net analyst , Germany one . But to enjoys s ' types 3 mln FISHMEAL currency Avg mln loss was yen in tonnes the qtr Fairchild may 4 former > Long pact seventies said dated to E yen statement marks 2 first expected - GETS after extraordinary , lower said tax the lt in the of Leeds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzlH_2nW-gyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9b6c2d3-6e1a-4d2b-ef84-ab34bf1ab6f9"
      },
      "source": [
        "# The probability of the text\n",
        "\n",
        "print(reduce(mul, [counts[w] for w in text], 1.0))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.137759495956524e-304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDctxYLcBooF"
      },
      "source": [
        "Compare the given probability above to the one you've previously guessed.\n",
        "\n",
        "Try changing the number of words and compare the results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftXjYNnKCMaT"
      },
      "source": [
        "# n-gram Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pIOV53YCPrb"
      },
      "source": [
        "Now let's construct a n-grams language model from the text.\n",
        "\n",
        "nltk already has functions for n-grams such as: `bigrams` & `trigrams`.\n",
        "\n",
        "Let's use them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZZzik2A_SZR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb072bd3-48dc-43a1-b2f9-6368cb118f12"
      },
      "source": [
        "first_sentence = reuters.sents()[0]\n",
        "print(first_sentence)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifXUjkxJ_518",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63bb4991-f977-4a4a-e267-99a11de9d5a1"
      },
      "source": [
        "# Get the bigrams\n",
        "print(list(bigrams(first_sentence)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWRQ3iJwASBT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ce2d1dc-224b-485f-cc1c-0f2dcfd32504"
      },
      "source": [
        "# Get the padded bigrams\n",
        "print(list(bigrams(first_sentence, pad_left=True, pad_right=True)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(None, 'ASIAN'), ('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.'), ('.', None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow0OeOo3AS7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a249bf8e-e9fd-4486-fc31-ab5be694176f"
      },
      "source": [
        "# Get the trigrams\n",
        "print(list(trigrams(first_sentence)))\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxnGokXfAT5N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a27557a2-85fb-4a9e-e96f-89faeba17480"
      },
      "source": [
        "# Get the padded trigrams\n",
        "print(list(trigrams(first_sentence, pad_left=True, pad_right=True)))\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(None, None, 'ASIAN'), (None, 'ASIAN', 'EXPORTERS'), ('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.'), ('said', '.', None), ('.', None, None)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmTKkqbJCjAQ"
      },
      "source": [
        "Let's use the trigrams on the Reuters corpus.\n",
        "\n",
        "We start by counting the occurences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CGdcS-wAVX0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f088f5e-1ecc-4914-df18-90c201530ffc"
      },
      "source": [
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        " \n",
        "for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model[(w1, w2)][w3] += 1\n",
        " \n",
        " \n",
        "print(model[\"what\", \"the\"][\"economists\"])\n",
        "print(model[\"what\", \"the\"][\"nonexistingword\"])\n",
        "print(model[None, None][\"The\"])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "0\n",
            "8839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_L2wmE4DLLd"
      },
      "source": [
        "And then converting it into frequencies or probabilities, by deviding these occurences by the total number of occurences of the first 2 words of our trigrams:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFVttrieC8RF"
      },
      "source": [
        "# Let's transform the counts to probabilities\n",
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5AlcTmgJs0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c0002e1-5da4-49bd-8695-6414eaa06b75"
      },
      "source": [
        "print(model[\"what\", \"the\"][\"economists\"])\n",
        "print(model[\"what\", \"the\"][\"nonexistingword\"])\n",
        "print(model[None, None][\"The\"])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.043478260869565216\n",
            "0.0\n",
            "0.16155800478879934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8F8Y2lVDqLV"
      },
      "source": [
        "---\n",
        "\n",
        "Now we're ready to try it out.\n",
        "\n",
        "Let's generate a random sentence again, but this time, we'll use our trigram model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhkojQA3Di_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e3a7ffc-da30-4df8-dadf-be68c6abc40d"
      },
      "source": [
        "text = [None, None]\n",
        "prob = 1.0 \n",
        " \n",
        "sentence_finished = False\n",
        " \n",
        "while not sentence_finished:\n",
        "    r = random.random()\n",
        "    accumulator = .0\n",
        " \n",
        "    for word in model[tuple(text[-2:])].keys():\n",
        "        accumulator += model[tuple(text[-2:])][word]\n",
        " \n",
        "        if accumulator >= r:\n",
        "            prob *= model[tuple(text[-2:])][word] \n",
        "            text.append(word)\n",
        "            break\n",
        " \n",
        "    if text[-2:] == [None, None]:\n",
        "        sentence_finished = True\n",
        " \n",
        "print(f'Probability of text={prob}')\n",
        "print(' '.join([t for t in text if t]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of text=1.0057756544669545e-21\n",
            "REEF ENERGY & lt ; BP > UNIT BUYS BENEFICIAL & lt ; ZBST > 3RD QTR NET Shr four cts vs two cts a share .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMQG202UD0aD"
      },
      "source": [
        "Try running the previous cell several times, and compare the output sentences to the previous random ones.\n",
        "\n",
        "> Which one looks better to you?\n",
        " \n",
        "> Can you explain why?\n",
        "\n",
        "\n",
        "Note that we have not used here complicated RNN or LSTM, and still managed to generate reasonable sentences, using only simple probability, and counting words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2CyR3qoFt1G"
      },
      "source": [
        "---\n",
        "\n",
        "## Exercise\n",
        "\n",
        "Your Turn:\n",
        "\n",
        "### Task 1\n",
        "- Optional: use your own corpus\n",
        "- Create a function that parse the text into 4-grams\n",
        "- Train a language model using the 4-grmams and\n",
        "- Generate few sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31Ht7Wu44mY4"
      },
      "source": [
        "Q: How can you use your language model as a spelling or grammar checker?\n",
        "\n",
        "Q2: What steps are needed to create a spelling or grammar correction suggestion using your language model? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKlyMas_4kBh"
      },
      "source": [
        "\n",
        "### Task 2:\n",
        "- Can you improve the words generation even further? \n",
        "\n",
        "  Hint: Check the usage of the `random.random()` - what is it for? Can you come up with a better alternative? \n",
        "\n",
        "- Do the new sentences look better? Do they make more sense?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZySTIWy3Dn14"
      },
      "source": [],
      "execution_count": 17,
      "outputs": []
    }
  ]
}