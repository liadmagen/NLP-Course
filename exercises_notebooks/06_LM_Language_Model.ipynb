{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_LM_Language_Model",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHpU4n2ZAd2p"
      },
      "source": [
        "# Language Model Example\n",
        "\n",
        "In this notebook, you'll train a n-gram Language model yourself"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYs_Dhab9syd"
      },
      "source": [
        "This example is based on:\n",
        "\n",
        "https://nlpforhackers.io/language-models/\n",
        "\n",
        "For a (very!) detailed information about this topic, please refer to:\n",
        "https://web.stanford.edu/~jurafsky/slp3/3.pdf "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxk8CHMfAr1H"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piIx18HE_dlq",
        "outputId": "93767cc6-018a-49ec-d0f0-7fdabe4b6b1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjCnv0889m7r"
      },
      "source": [
        "import random\n",
        "from functools import reduce\n",
        "\n",
        "from operator import mul\n",
        "from collections import Counter\n",
        "\n",
        "from nltk.corpus import reuters\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj4-vpdmAuLu"
      },
      "source": [
        "# Bag-of-words & Frequency calculation\n",
        "\n",
        "We start by constructing a Bag-of-words.\n",
        "\n",
        "Remind yourself that a Language Model is a calculation of the frequencies and the probabilities of words, based on their observed occurrences.\n",
        "\n",
        "We calculate the word frequencies, by simply counting the word occurrences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sI732WCZ9wsv",
        "outputId": "8787ad15-c015-439d-a544-b78102a116e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "counts = Counter(reuters.words())\n",
        "total_count = len(reuters.words())\n",
        "\n",
        "print(counts.most_common(n=20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('.', 94687), (',', 72360), ('the', 58251), ('of', 35979), ('to', 34035), ('in', 26478), ('said', 25224), ('and', 25043), ('a', 23492), ('mln', 18037), ('vs', 14120), ('-', 13705), ('for', 12785), ('dlrs', 11730), (\"'\", 11272), ('The', 10968), ('000', 10277), ('1', 9977), ('s', 9298), ('pct', 9093)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnxBoLt6-Rsz",
        "outputId": "8a63f57b-4fd7-4509-f02d-54418e5a9558",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Compute the frequencies\n",
        "for word in counts:\n",
        "    counts[word] /= float(total_count)\n",
        "\n",
        "# The frequencies should add up to 1\n",
        "print(sum(counts.values()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0000000000006808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pzx-cyghBJzA"
      },
      "source": [
        "Let's create a random text passage, and calculate what is the probability that such a sentence is valid.\n",
        "\n",
        "Before continuing, and executing the next cells: \n",
        "\n",
        "**please pause and guess, what do you expect that the probability of a random 5 words passage would be? How about a 100?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4FefXrB9_YN",
        "outputId": "c9240f39-ca4c-440f-ec8b-ce6927ee6ff6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        " # Generate 100 words of language\n",
        "text = []\n",
        " \n",
        "for _ in range(100):\n",
        "    r = random.random()\n",
        "    accumulator = .0\n",
        " \n",
        "    for word, freq in counts.items():\n",
        "        accumulator += freq\n",
        " \n",
        "        if accumulator >= r:\n",
        "            text.append(word)\n",
        "            break\n",
        " \n",
        "print(' '.join(text))\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "movements S it Harbour stock - - vs profit SHARES ACQUIRES at the / cts got said 5 net financial mln 1 March of and & don 16 mln remedial the speculated stabilisation ' CYCLOPS information days 6 quite .\" The Means strikes 7 hour members said & Icahn the 08 officials the said likely 300 taken two lie include 755 31 Flank > and terms from be in privately 7 prices in public Excluding Year optimistic want figure , been of mln rose of . orders , MAD BUSINESS tax . pct the market about investment Beaver . 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzlH_2nW-gyQ",
        "outputId": "f49c95db-080b-419a-94ec-77860478c44e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# The probability of the text\n",
        "\n",
        "print(reduce(mul, [counts[w] for w in text], 1.0))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5910690708843e-310\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDctxYLcBooF"
      },
      "source": [
        "Compare the given probability above to the one you've previously guessed.\n",
        "\n",
        "Try changing the number of words and compare the results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftXjYNnKCMaT"
      },
      "source": [
        "# n-gram Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pIOV53YCPrb"
      },
      "source": [
        "Now let's construct a n-grams language model from the text.\n",
        "\n",
        "nltk already has functions for n-grams such as: `bigrams` & `trigrams`.\n",
        "\n",
        "Let's use them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZZzik2A_SZR",
        "outputId": "fbe5bf38-ab5f-4c8b-dda0-d2a7ecbc78b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "first_sentence = reuters.sents()[0]\n",
        "print(first_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifXUjkxJ_518",
        "outputId": "ca924551-034d-4158-ad70-c1deb36e586f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the bigrams\n",
        "print(list(bigrams(first_sentence)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWRQ3iJwASBT",
        "outputId": "1c1c361f-db17-45d9-ce59-40996051dbc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the padded bigrams\n",
        "print(list(bigrams(first_sentence, pad_left=True, pad_right=True)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(None, 'ASIAN'), ('ASIAN', 'EXPORTERS'), ('EXPORTERS', 'FEAR'), ('FEAR', 'DAMAGE'), ('DAMAGE', 'FROM'), ('FROM', 'U'), ('U', '.'), ('.', 'S'), ('S', '.-'), ('.-', 'JAPAN'), ('JAPAN', 'RIFT'), ('RIFT', 'Mounting'), ('Mounting', 'trade'), ('trade', 'friction'), ('friction', 'between'), ('between', 'the'), ('the', 'U'), ('U', '.'), ('.', 'S'), ('S', '.'), ('.', 'And'), ('And', 'Japan'), ('Japan', 'has'), ('has', 'raised'), ('raised', 'fears'), ('fears', 'among'), ('among', 'many'), ('many', 'of'), ('of', 'Asia'), ('Asia', \"'\"), (\"'\", 's'), ('s', 'exporting'), ('exporting', 'nations'), ('nations', 'that'), ('that', 'the'), ('the', 'row'), ('row', 'could'), ('could', 'inflict'), ('inflict', 'far'), ('far', '-'), ('-', 'reaching'), ('reaching', 'economic'), ('economic', 'damage'), ('damage', ','), (',', 'businessmen'), ('businessmen', 'and'), ('and', 'officials'), ('officials', 'said'), ('said', '.'), ('.', None)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow0OeOo3AS7R",
        "outputId": "0959f7af-9d83-48f8-9605-1b3a2764995f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the trigrams\n",
        "print(list(trigrams(first_sentence)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxnGokXfAT5N",
        "outputId": "df635f0b-544e-4c70-be22-638c30ca2ffa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the padded trigrams\n",
        "print(list(trigrams(first_sentence, pad_left=True, pad_right=True)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(None, None, 'ASIAN'), (None, 'ASIAN', 'EXPORTERS'), ('ASIAN', 'EXPORTERS', 'FEAR'), ('EXPORTERS', 'FEAR', 'DAMAGE'), ('FEAR', 'DAMAGE', 'FROM'), ('DAMAGE', 'FROM', 'U'), ('FROM', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.-'), ('S', '.-', 'JAPAN'), ('.-', 'JAPAN', 'RIFT'), ('JAPAN', 'RIFT', 'Mounting'), ('RIFT', 'Mounting', 'trade'), ('Mounting', 'trade', 'friction'), ('trade', 'friction', 'between'), ('friction', 'between', 'the'), ('between', 'the', 'U'), ('the', 'U', '.'), ('U', '.', 'S'), ('.', 'S', '.'), ('S', '.', 'And'), ('.', 'And', 'Japan'), ('And', 'Japan', 'has'), ('Japan', 'has', 'raised'), ('has', 'raised', 'fears'), ('raised', 'fears', 'among'), ('fears', 'among', 'many'), ('among', 'many', 'of'), ('many', 'of', 'Asia'), ('of', 'Asia', \"'\"), ('Asia', \"'\", 's'), (\"'\", 's', 'exporting'), ('s', 'exporting', 'nations'), ('exporting', 'nations', 'that'), ('nations', 'that', 'the'), ('that', 'the', 'row'), ('the', 'row', 'could'), ('row', 'could', 'inflict'), ('could', 'inflict', 'far'), ('inflict', 'far', '-'), ('far', '-', 'reaching'), ('-', 'reaching', 'economic'), ('reaching', 'economic', 'damage'), ('economic', 'damage', ','), ('damage', ',', 'businessmen'), (',', 'businessmen', 'and'), ('businessmen', 'and', 'officials'), ('and', 'officials', 'said'), ('officials', 'said', '.'), ('said', '.', None), ('.', None, None)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmTKkqbJCjAQ"
      },
      "source": [
        "Let's use the trigrams on the Reuters corpus.\n",
        "\n",
        "We start by counting the occurences:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CGdcS-wAVX0",
        "outputId": "d51dc395-d495-40c1-f653-9cdd33dfaa4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        " \n",
        "for sentence in reuters.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model[(w1, w2)][w3] += 1\n",
        " \n",
        " \n",
        "print(model[\"what\", \"the\"][\"economists\"])\n",
        "print(model[\"what\", \"the\"][\"nonexistingword\"])\n",
        "print(model[None, None][\"The\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "0\n",
            "8839\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_L2wmE4DLLd"
      },
      "source": [
        "And then converting it into frequencies or probabilities, by deviding these occurences by the total number of occurences of the first 2 words of our trigrams:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFVttrieC8RF"
      },
      "source": [
        "# Let's transform the counts to probabilities\n",
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5AlcTmgJs0T",
        "outputId": "82667e6d-1d82-4b6e-86e4-22e8a1a71f03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(model[\"what\", \"the\"][\"economists\"])\n",
        "print(model[\"what\", \"the\"][\"nonexistingword\"])\n",
        "print(model[None, None][\"The\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.043478260869565216\n",
            "0.0\n",
            "0.16154324146501936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8F8Y2lVDqLV"
      },
      "source": [
        "And now we're ready to try it out.\n",
        "\n",
        "Let's generate a random sentence again, but this time, we'll use our trigram model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhkojQA3Di_p",
        "outputId": "81886e44-9dc2-41da-e25b-29b9583b5629",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text = [None, None]\n",
        "prob = 1.0 \n",
        " \n",
        "sentence_finished = False\n",
        " \n",
        "while not sentence_finished:\n",
        "    r = random.random()\n",
        "    accumulator = .0\n",
        " \n",
        "    for word in model[tuple(text[-2:])].keys():\n",
        "        accumulator += model[tuple(text[-2:])][word]\n",
        " \n",
        "        if accumulator >= r:\n",
        "            prob *= model[tuple(text[-2:])][word] \n",
        "            text.append(word)\n",
        "            break\n",
        " \n",
        "    if text[-2:] == [None, None]:\n",
        "        sentence_finished = True\n",
        " \n",
        "print(f'Probability of text={prob}')\n",
        "print(' '.join([t for t in text if t]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability of text=2.0872967232657588e-57\n",
            "January ' s economic outlook also attended by representatives from 151 , 000 Revs 6 , 271 , 000 vs 2 . 75 dlrs in excess of its Geneva , Switzerland , which accounted for just under one pct increase in profitability by midyear , which will improve prospects for the block was Eduardo Cojuanco , the department said .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMQG202UD0aD"
      },
      "source": [
        "Try running this cell several times, and compare the output sentences to the previous random ones.\n",
        "\n",
        "> Which one looks better to you?\n",
        " \n",
        "> Can you explain why?\n",
        "\n",
        "\n",
        "Note that we have not used here complicated RNN or LSTM, and still managed to generate reasonable sentences, using only simple probability, and counting words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2CyR3qoFt1G"
      },
      "source": [
        "---\n",
        "\n",
        "Your Turn:\n",
        "- Optional: use your own corpus\n",
        "- Create a function that parse the text into 4-grams\n",
        "- Train a language model using the 4-grmams and\n",
        "- Generate few sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31Ht7Wu44mY4"
      },
      "source": [
        "Q: How can you use your language model as a spelling or grammar checker?\n",
        "\n",
        "Q2: What steps are needed to create a spelling or grammar correction suggestion using your language model? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKlyMas_4kBh"
      },
      "source": [
        "\n",
        "Task 2:\n",
        "- For those of you who have finished it quickly, can you improve the generation even further? Hint: currently the comparisson is above a random threshold...\n",
        "\n",
        "> Do they look better? Do they make more sense?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZySTIWy3Dn14"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}