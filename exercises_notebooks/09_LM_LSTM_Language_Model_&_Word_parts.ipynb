{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09_LM_LSTM Language Model & Word-parts.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "531c4031758c49ba92607befcdc8d312": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_82939dcf9c1b48f5814a76293dea4eaa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_77780c380c7f4aed8a539d794e8cc1e5",
              "IPY_MODEL_cd24062d23444afc81b8eecc98c103b6",
              "IPY_MODEL_3356680e8be443158036ecf03511e310"
            ]
          }
        },
        "82939dcf9c1b48f5814a76293dea4eaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "77780c380c7f4aed8a539d794e8cc1e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a33f7ac79cc24b8085ea68dade29b82a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Validation sanity check:   0%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a91b2e1a7e094030bf72c4a32edea091"
          }
        },
        "cd24062d23444afc81b8eecc98c103b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_82e901e49012475da63fbf558760d7a2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bbd077c1cbc14080bc719f8f84b3a00a"
          }
        },
        "3356680e8be443158036ecf03511e310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ca2477f0a59740a39e70c2293db11621",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/2 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3c0b752b2eb247ffb2b55b904fb97998"
          }
        },
        "a33f7ac79cc24b8085ea68dade29b82a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a91b2e1a7e094030bf72c4a32edea091": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "82e901e49012475da63fbf558760d7a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bbd077c1cbc14080bc719f8f84b3a00a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ca2477f0a59740a39e70c2293db11621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3c0b752b2eb247ffb2b55b904fb97998": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "97f1ad50a0174d3a895a2659d4ea00a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4154ae5ae9704e57b4594edf9ebdeb2f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3bfae2b2aa9e49dbbbd3362b06bf6621",
              "IPY_MODEL_663d692033524475821f1c7bb2c33bac",
              "IPY_MODEL_6e229851ec8f46408c99526fe2218189"
            ]
          }
        },
        "4154ae5ae9704e57b4594edf9ebdeb2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "3bfae2b2aa9e49dbbbd3362b06bf6621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c9ab56cdf8424538a4f02743b3445966",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch 1:  38%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b03d8f2a7efd443c951ebd6ade35be73"
          }
        },
        "663d692033524475821f1c7bb2c33bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c0705c356efb45e38b3fbe3523b8345b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 10258,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3900,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f47bb46d42284626a3218f5b8a72a510"
          }
        },
        "6e229851ec8f46408c99526fe2218189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2f0847555060472cb4feaf334f6fa602",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3880/10258 [03:34&lt;05:53, 18.05it/s, loss=6.38, v_num=4]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2c4344e6fde34d98b315ff823811ec5b"
          }
        },
        "c9ab56cdf8424538a4f02743b3445966": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b03d8f2a7efd443c951ebd6ade35be73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0705c356efb45e38b3fbe3523b8345b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f47bb46d42284626a3218f5b8a72a510": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f0847555060472cb4feaf334f6fa602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2c4344e6fde34d98b315ff823811ec5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liadmagen/NLP-Course/blob/master/exercises_notebooks/09_LM_LSTM_Language_Model_%26_Word_parts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5zwGMzVD0pF"
      },
      "source": [
        "# RNN / BiLSTM Language Model using Word Vectors\n",
        "In this notebook, we will train a language model using **LSTM**.\n",
        "\n",
        "We'll load the frankenstein book, and convert it into semantic representation through a word vectors library called BytePair Embedding (BPEmb).\n",
        "\n",
        "After you run this notebook, please try changing the data-source from \"frankenstein.txt\" to \"dracula.txt\", and observe the result. \n",
        "\n",
        "### How are we going to do it?\n",
        "\n",
        "We will define our data, as such, that for every word we use as an input for the model $(X=W_n)$, the next word would be the output $(Y = W_{n+1})$\n",
        "\n",
        "The words in the output, Y, will be represented as a one-hot-vector. \n",
        "\n",
        "**Q: What is the size of this Vector?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NTgD9mVh-gi"
      },
      "source": [
        "%%capture\n",
        "!pip install bpemb"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrUhaljjLeco"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import math\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, tensor\n",
        "\n",
        "from torchtext.data import get_tokenizer\n",
        "\n",
        "from bpemb import BPEmb"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYT3W-Rekse4"
      },
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSl2MJaLD-au"
      },
      "source": [
        "## Word Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spXr-wu8D8P2"
      },
      "source": [
        "Let's convert the text into vectors.\n",
        "\n",
        "We will use a package called [BPEmb](https://nlp.h-its.org/bpemb/) which encodes words to vectors by dividing these words to **sub-words**, pieces of words, made of characters which frequently appear together.\n",
        "\n",
        "Q: Remember what is the name of the Linguistic level that deals with letter-level? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgKbDV-5qAXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "318fb05b-8d79-4428-bc55-f1a70ebc25e8"
      },
      "source": [
        "bpemb_en = BPEmb(lang=\"en\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400869/400869 [00:00<00:00, 722911.23B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.d100.w2v.bin.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3784656/3784656 [00:01<00:00, 3369678.16B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnWzITC8qCvw",
        "outputId": "9a402f3c-f302-4529-ed42-366d9426902f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bpemb_en.vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaV2IRzjEClD"
      },
      "source": [
        "We will use this helper function to load the corpus data (the books):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVl_hJ8yNktz"
      },
      "source": [
        "def get_file(filename = \"frankenstein.txt\"):\n",
        "  path = tf.keras.utils.get_file(\n",
        "      filename, origin=f\"https://raw.githubusercontent.com/liadmagen/NLP-Course/master/dataset/{filename}\"\n",
        "  )\n",
        "  with open(path, encoding=\"utf-8\") as f:\n",
        "      text = f.read() \n",
        "  text = text.replace(\"\\n\", \" \")        # Remove line-breaks & newlines\n",
        "  print(\"Corpus length:\", len(text))\n",
        "  return text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2iZ2Ol3EbU9"
      },
      "source": [
        "## RNN Model\n",
        "And this is the model itself. This is a very raw structure of it. \n",
        "\n",
        "Note: In 'real-ilfe' we're using helping frameworks such as [ignite](https://pytorch.org/ignite/) or [lightning](https://www.pytorchlightning.ai/). \n",
        "\n",
        "We bring it in this version here, for learning purposes only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a768GRY4TXVH"
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ninp, noutp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "          ninp =  LSTM input size \n",
        "          noutp = size of the output (number of classes)\n",
        "          nhid = number of neurons in the hidden layer\n",
        "          nlayers = number of hidden layer\n",
        "          dropout = dropout rate\n",
        "          tie_weights = whether to use tie_weights (see note)\n",
        "        \"\"\"\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.noutp = noutp\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.encoder = nn.Embedding.from_pretrained(tensor(bpemb_en.vectors))\n",
        "        \n",
        "        # self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity='relu', dropout=dropout)\n",
        "\n",
        "        # LSTM (Long Short-Term Memory) is a version of RNN that 'remembers' \n",
        "        # the previous steps and therefore converges better on longer sequences. \n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "\n",
        "        self.decoder = nn.Linear(nhid, noutp)\n",
        "\n",
        "        # Optionally tie weights as in:\n",
        "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
        "        # https://arxiv.org/abs/1608.05859\n",
        "        # and\n",
        "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
        "        # https://arxiv.org/abs/1611.01462\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to ninp (embedding size)')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.weight)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.noutp)\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, batch_size, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, batch_size, self.nhid))\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsVkK0WRE4us"
      },
      "source": [
        "A helper class to convert the tokens into batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cc83Hwah0Hc"
      },
      "source": [
        "def batchify(data, batch_size):\n",
        "    # Work out how cleanly we can divide the dataset into batch_size parts.\n",
        "    nbatch = data.size(0) // batch_size\n",
        "\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    \n",
        "    # Evenly divide the data across the batch_size batches.\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    \n",
        "    return data.to(device)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFFTghmZE8JO"
      },
      "source": [
        "Let's load the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJq1xx_Lj813",
        "outputId": "230caae0-e7cb-4ca3-d529-309a5445f5ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_corpus = get_file('dracula.txt')\n",
        "val_corpus = get_file('frankenstein.txt')\n",
        "\n",
        "print(train_corpus[:300])\n",
        "print(val_corpus[:300])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/liadmagen/NLP-Course/master/dataset/dracula.txt\n",
            "860160/857524 [==============================] - 0s 0us/step\n",
            "868352/857524 [==============================] - 0s 0us/step\n",
            "Corpus length: 842159\n",
            "Downloading data from https://raw.githubusercontent.com/liadmagen/NLP-Course/master/dataset/frankenstein.txt\n",
            "434176/430265 [==============================] - 0s 0us/step\n",
            "442368/430265 [==============================] - 0s 0us/step\n",
            "Corpus length: 420726\n",
            "Dracula, by Bram Stoker  CHAPTER I  JONATHAN HARKER'S JOURNAL  (_Kept in shorthand._)   _3 May. Bistritz._--Left Munich at 8:35 P. M., on 1st May, arriving at Vienna early next morning; should have arrived at 6:46, but train was an hour late. Buda-Pesth seems a wonderful place, from the glimpse whic\n",
            "Frankenstein, or, the Modern Prometheus by Mary Wollstonecraft (Godwin) Shelley  Letter 1  _To Mrs. Saville, England._   St. Petersburgh, Dec. 11th, 17—.   You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rA2YyF8E9-5"
      },
      "source": [
        "## Semantic representation + word-parts\n",
        "\n",
        "And convert it into vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC4A4ygqiYDd"
      },
      "source": [
        "train_encoded_text = bpemb_en.encode(train_corpus)\n",
        "train_encoded_ids = bpemb_en.encode_ids(train_corpus)\n",
        "\n",
        "val_encoded_text = bpemb_en.encode(val_corpus)\n",
        "val_encoded_ids = bpemb_en.encode_ids(val_corpus)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXmw96xvFEzL"
      },
      "source": [
        "Let's check the result of encoded_text (we'll get to encoded_ids in a moment).\n",
        "\n",
        "Notice that every word is now broken to pieces. \n",
        "\n",
        "A **'_'** mark in the beginning of a token, represents a beginning of a new word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oki7xTl5FDyT",
        "outputId": "624f34d1-7a64-49bd-98df-467a7c23123e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_encoded_text[:50]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁dra',\n",
              " 'c',\n",
              " 'ula',\n",
              " ',',\n",
              " '▁by',\n",
              " '▁br',\n",
              " 'am',\n",
              " '▁st',\n",
              " 'oker',\n",
              " '▁chapter',\n",
              " '▁i',\n",
              " '▁jonathan',\n",
              " '▁har',\n",
              " 'ker',\n",
              " \"'\",\n",
              " 's',\n",
              " '▁journal',\n",
              " '▁(',\n",
              " '_',\n",
              " 'ke',\n",
              " 'pt',\n",
              " '▁in',\n",
              " '▁sh',\n",
              " 'or',\n",
              " 'th',\n",
              " 'and',\n",
              " '.',\n",
              " '_',\n",
              " ')',\n",
              " '▁',\n",
              " '_',\n",
              " '0',\n",
              " '▁may',\n",
              " '.',\n",
              " '▁b',\n",
              " 'ist',\n",
              " 'rit',\n",
              " 'z',\n",
              " '.',\n",
              " '_',\n",
              " '-',\n",
              " '-',\n",
              " 'left',\n",
              " '▁mun',\n",
              " 'ich',\n",
              " '▁at',\n",
              " '▁0:00',\n",
              " '▁p',\n",
              " '.',\n",
              " '▁m']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkpMGwmYFR8E"
      },
      "source": [
        "This method is called **word-parts**. \n",
        "\n",
        "Instead of converting whole words (word2vec, gloVe), or characters (FastText), this method converts slices of text, frequent combinations of characters, which often appear together in the text.\n",
        "\n",
        "It does so by finding the most common and frequent combinations of characters in a very big corpus, counting how many each of those appear and selecting the top K combinations. Pay attention that those combinations may sometimes be just letters: `st`, `z`, or words `journal`, `chapter`. \n",
        "\n",
        "The result is having a vocabulary which is WAY smaller than all-the-words-in-a-language (how big would that be for English, for example? How about for your native language?) but bigger than a vocabulary that includes all the characters of a language (or multiple languages):\n",
        "\n",
        "**character-based << word-piece based << word-based**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21-E5O3tF9HK"
      },
      "source": [
        "## Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGD8BiR7oT11"
      },
      "source": [
        "batch_size = 32\n",
        "eval_batch_size = 32\n",
        "\n",
        "vocab_size = bpemb_en.vocab_size\n",
        "embsize = bpemb_en.vectors.shape[1]\n",
        "nhidden = 256\n",
        "nlayers = 2"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAdLH-8YuILA"
      },
      "source": [
        "model = RNNModel(embsize, vocab_size, nhidden, nlayers).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2g7-0EkqYZh"
      },
      "source": [
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ9zbXUwGBXd"
      },
      "source": [
        "## Division to train/validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyj2AlPwuLui"
      },
      "source": [
        "train_enc_ids = torch.tensor(train_encoded_ids).type(torch.int64)\n",
        "train_data = batchify(train_enc_ids, batch_size)\n",
        "\n",
        "val_enc_ids = torch.tensor(val_encoded_ids).type(torch.int64)\n",
        "val_data = batchify(val_enc_ids, batch_size)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After batchify, our text is divided into fixed-size batches of word indices."
      ],
      "metadata": {
        "id": "sLXbRHg64nlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"training data size: \", train_data.shape)\n",
        "train_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBSo3doE4kbT",
        "outputId": "306dc908-b974-436b-baa0-88e9c0f9b555"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training data size:  torch.Size([6972, 32])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1187,    7, 4003,  ...,  107,  204, 9948],\n",
              "        [9924, 3027, 9935,  ...,   42, 9937, 9940],\n",
              "        [2206, 1274,    7,  ...,  619, 9920, 9940],\n",
              "        ...,\n",
              "        [1842, 7579,   27,  ...,    7, 1597,   72],\n",
              "        [6732,   71, 4280,  ...,   91,  107,  335],\n",
              "        [ 544,    7,  154,  ...,  363,   73,   10]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaDDZuY7qyRH"
      },
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We return the current batch (current words) and the next one as the target (the n+1 words)."
      ],
      "metadata": {
        "id": "HV4TIgm2Gi4k"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj9TGJMkypM9"
      },
      "source": [
        "def get_batch(source, i):\n",
        "    seq_len = min(batch_size, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "curr_batch, next_batch = get_batch(train_data, 0)\n",
        "print(\"Current sentence IDs: \", curr_batch[0])\n",
        "print(\"Target token id:\", next_batch[0].item())\n",
        "\n",
        "print(\"Current sentence: \", bpemb_en.decode_ids(curr_batch[0]))\n",
        "print(\"Target token: \", bpemb_en.decode_ids([next_batch[0].item()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CxQV2lsGuBa",
        "outputId": "ac1336b1-0e42-499b-f1ba-1599ccde1a49"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current sentence IDs:  tensor([1187,    7, 4003, 4062, 9951,   34,  142,  120, 9976, 1022, 6948,  437,\n",
            "         280,    7,   25,  394, 3238,  352, 7607, 7081, 7253,   58, 9934, 1233,\n",
            "        9934,  822,  335, 1375, 3677,  107,  204, 9948], device='cuda:0')\n",
            "Target token id: 9924\n",
            "Current sentence:  dra the table things; andies that_ life?\" may can theit off professor mesing everythingumed th,let, go her inf quickly itire:\n",
            "Target token:  c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-fF-d0xIIYlW",
        "outputId": "a4f769d9-613a-45ac-c120-d7b1f918db59"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'dra the table things; andies that_ life?\" may can theit off professor mesing everythingumed th,let, go her inf quickly itire:'"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdvF1m90GGtJ"
      },
      "source": [
        "## Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLQmqGbuq39-"
      },
      "source": [
        "def train(train_data, log_interval = 100):\n",
        "    # Turn on training mode - which enables dropout.\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0.\n",
        "\n",
        "    start_time = time.time()\n",
        "    ntokens = len(train_data)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, batch_size)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        model.zero_grad()\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "        for p in model.parameters():\n",
        "          if p.grad is not None:\n",
        "            p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // batch_size, lr,\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REFFlXWO0MbI"
      },
      "source": [
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(data_source)\n",
        "\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, batch_size):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvhCcxsTGPk0"
      },
      "source": [
        "## Training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSd0s9jUuA3O",
        "outputId": "0d470161-74bc-47ff-c348-30d0ccef46f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Loop over epochs.\n",
        "lr = 20\n",
        "best_val_loss = None\n",
        "epochs = 40\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_data)\n",
        "    val_loss = evaluate(val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "            'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                        val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "    else:\n",
        "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "        lr /= 2.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   100/  217 batches | lr 20.00 | ms/batch 31.28 | loss  7.23 | ppl  1386.43\n",
            "| epoch   1 |   200/  217 batches | lr 20.00 | ms/batch 28.21 | loss  6.63 | ppl   754.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  7.27s | valid loss  7.04 | valid ppl  1140.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   100/  217 batches | lr 20.00 | ms/batch 28.46 | loss  6.58 | ppl   722.67\n",
            "| epoch   2 |   200/  217 batches | lr 20.00 | ms/batch 28.19 | loss  6.46 | ppl   636.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  6.99s | valid loss  6.94 | valid ppl  1032.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   100/  217 batches | lr 20.00 | ms/batch 28.50 | loss  6.46 | ppl   641.77\n",
            "| epoch   3 |   200/  217 batches | lr 20.00 | ms/batch 28.19 | loss  6.34 | ppl   567.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time:  7.00s | valid loss  6.94 | valid ppl  1030.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   100/  217 batches | lr 20.00 | ms/batch 28.52 | loss  6.33 | ppl   561.91\n",
            "| epoch   4 |   200/  217 batches | lr 20.00 | ms/batch 28.26 | loss  6.16 | ppl   473.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time:  7.01s | valid loss  6.77 | valid ppl   868.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   100/  217 batches | lr 20.00 | ms/batch 28.53 | loss  6.15 | ppl   469.02\n",
            "| epoch   5 |   200/  217 batches | lr 20.00 | ms/batch 28.19 | loss  6.00 | ppl   403.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time:  7.00s | valid loss  6.66 | valid ppl   777.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   100/  217 batches | lr 20.00 | ms/batch 28.55 | loss  6.00 | ppl   404.26\n",
            "| epoch   6 |   200/  217 batches | lr 20.00 | ms/batch 28.21 | loss  5.88 | ppl   356.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time:  7.01s | valid loss  6.65 | valid ppl   770.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   100/  217 batches | lr 20.00 | ms/batch 28.60 | loss  5.89 | ppl   363.04\n",
            "| epoch   7 |   200/  217 batches | lr 20.00 | ms/batch 28.24 | loss  5.77 | ppl   319.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time:  7.02s | valid loss  6.55 | valid ppl   698.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   100/  217 batches | lr 20.00 | ms/batch 28.60 | loss  5.78 | ppl   325.31\n",
            "| epoch   8 |   200/  217 batches | lr 20.00 | ms/batch 28.24 | loss  5.69 | ppl   295.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time:  7.02s | valid loss  6.50 | valid ppl   664.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   100/  217 batches | lr 20.00 | ms/batch 28.60 | loss  5.71 | ppl   303.10\n",
            "| epoch   9 |   200/  217 batches | lr 20.00 | ms/batch 28.26 | loss  5.61 | ppl   273.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time:  7.02s | valid loss  6.48 | valid ppl   652.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   100/  217 batches | lr 20.00 | ms/batch 28.53 | loss  5.64 | ppl   281.35\n",
            "| epoch  10 |   200/  217 batches | lr 20.00 | ms/batch 28.23 | loss  5.55 | ppl   256.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time:  7.01s | valid loss  6.47 | valid ppl   642.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   100/  217 batches | lr 20.00 | ms/batch 28.55 | loss  5.58 | ppl   265.20\n",
            "| epoch  11 |   200/  217 batches | lr 20.00 | ms/batch 28.20 | loss  5.49 | ppl   243.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time:  7.01s | valid loss  6.44 | valid ppl   625.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   100/  217 batches | lr 20.00 | ms/batch 28.53 | loss  5.52 | ppl   249.59\n",
            "| epoch  12 |   200/  217 batches | lr 20.00 | ms/batch 28.23 | loss  5.43 | ppl   228.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time:  7.01s | valid loss  6.40 | valid ppl   602.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   100/  217 batches | lr 20.00 | ms/batch 28.56 | loss  5.47 | ppl   237.03\n",
            "| epoch  13 |   200/  217 batches | lr 20.00 | ms/batch 28.29 | loss  5.39 | ppl   218.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time:  7.02s | valid loss  6.38 | valid ppl   592.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   100/  217 batches | lr 20.00 | ms/batch 28.61 | loss  5.42 | ppl   226.26\n",
            "| epoch  14 |   200/  217 batches | lr 20.00 | ms/batch 28.19 | loss  5.34 | ppl   208.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time:  7.02s | valid loss  6.38 | valid ppl   588.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   100/  217 batches | lr 20.00 | ms/batch 28.58 | loss  5.38 | ppl   216.80\n",
            "| epoch  15 |   200/  217 batches | lr 20.00 | ms/batch 28.29 | loss  5.30 | ppl   199.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time:  7.03s | valid loss  6.36 | valid ppl   575.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   100/  217 batches | lr 20.00 | ms/batch 28.59 | loss  5.34 | ppl   207.48\n",
            "| epoch  16 |   200/  217 batches | lr 20.00 | ms/batch 28.22 | loss  5.26 | ppl   192.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time:  7.02s | valid loss  6.33 | valid ppl   562.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   100/  217 batches | lr 20.00 | ms/batch 28.55 | loss  5.30 | ppl   200.08\n",
            "| epoch  17 |   200/  217 batches | lr 20.00 | ms/batch 28.23 | loss  5.22 | ppl   185.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time:  7.01s | valid loss  6.33 | valid ppl   562.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   100/  217 batches | lr 20.00 | ms/batch 28.51 | loss  5.26 | ppl   192.53\n",
            "| epoch  18 |   200/  217 batches | lr 20.00 | ms/batch 28.23 | loss  5.19 | ppl   179.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time:  7.01s | valid loss  6.31 | valid ppl   549.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   100/  217 batches | lr 20.00 | ms/batch 28.59 | loss  5.23 | ppl   186.02\n",
            "| epoch  19 |   200/  217 batches | lr 20.00 | ms/batch 28.22 | loss  5.15 | ppl   172.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time:  7.02s | valid loss  6.30 | valid ppl   546.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   100/  217 batches | lr 20.00 | ms/batch 28.52 | loss  5.20 | ppl   181.34\n",
            "| epoch  20 |   200/  217 batches | lr 20.00 | ms/batch 28.22 | loss  5.12 | ppl   167.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time:  7.01s | valid loss  6.31 | valid ppl   550.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   100/  217 batches | lr 10.00 | ms/batch 28.53 | loss  5.12 | ppl   168.01\n",
            "| epoch  21 |   200/  217 batches | lr 10.00 | ms/batch 28.24 | loss  5.02 | ppl   152.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time:  7.01s | valid loss  6.29 | valid ppl   536.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   100/  217 batches | lr 10.00 | ms/batch 28.56 | loss  5.08 | ppl   160.64\n",
            "| epoch  22 |   200/  217 batches | lr 10.00 | ms/batch 28.22 | loss  5.01 | ppl   149.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time:  7.01s | valid loss  6.27 | valid ppl   530.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   100/  217 batches | lr 10.00 | ms/batch 28.56 | loss  5.06 | ppl   157.12\n",
            "| epoch  23 |   200/  217 batches | lr 10.00 | ms/batch 28.28 | loss  4.98 | ppl   145.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time:  7.02s | valid loss  6.27 | valid ppl   528.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   100/  217 batches | lr 10.00 | ms/batch 28.52 | loss  5.05 | ppl   155.31\n",
            "| epoch  24 |   200/  217 batches | lr 10.00 | ms/batch 28.21 | loss  4.96 | ppl   143.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time:  7.00s | valid loss  6.27 | valid ppl   527.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   100/  217 batches | lr 10.00 | ms/batch 28.49 | loss  5.02 | ppl   151.97\n",
            "| epoch  25 |   200/  217 batches | lr 10.00 | ms/batch 28.22 | loss  4.95 | ppl   141.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time:  7.01s | valid loss  6.26 | valid ppl   525.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   100/  217 batches | lr 10.00 | ms/batch 28.53 | loss  5.01 | ppl   149.96\n",
            "| epoch  26 |   200/  217 batches | lr 10.00 | ms/batch 28.22 | loss  4.93 | ppl   138.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time:  7.01s | valid loss  6.26 | valid ppl   521.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   100/  217 batches | lr 10.00 | ms/batch 28.53 | loss  4.99 | ppl   147.25\n",
            "| epoch  27 |   200/  217 batches | lr 10.00 | ms/batch 28.21 | loss  4.92 | ppl   137.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time:  7.01s | valid loss  6.25 | valid ppl   519.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   100/  217 batches | lr 10.00 | ms/batch 28.63 | loss  4.97 | ppl   144.19\n",
            "| epoch  28 |   200/  217 batches | lr 10.00 | ms/batch 28.26 | loss  4.90 | ppl   134.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time:  7.03s | valid loss  6.27 | valid ppl   527.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   100/  217 batches | lr 5.00 | ms/batch 28.44 | loss  4.94 | ppl   139.14\n",
            "| epoch  29 |   200/  217 batches | lr 5.00 | ms/batch 28.23 | loss  4.85 | ppl   128.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time:  7.01s | valid loss  6.25 | valid ppl   519.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   100/  217 batches | lr 2.50 | ms/batch 28.53 | loss  4.91 | ppl   135.45\n",
            "| epoch  30 |   200/  217 batches | lr 2.50 | ms/batch 28.26 | loss  4.82 | ppl   124.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time:  7.01s | valid loss  6.25 | valid ppl   518.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   100/  217 batches | lr 2.50 | ms/batch 28.57 | loss  4.89 | ppl   133.26\n",
            "| epoch  31 |   200/  217 batches | lr 2.50 | ms/batch 28.26 | loss  4.82 | ppl   123.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time:  7.02s | valid loss  6.25 | valid ppl   517.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   100/  217 batches | lr 2.50 | ms/batch 28.55 | loss  4.88 | ppl   132.28\n",
            "| epoch  32 |   200/  217 batches | lr 2.50 | ms/batch 28.21 | loss  4.81 | ppl   122.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time:  7.01s | valid loss  6.24 | valid ppl   515.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   100/  217 batches | lr 2.50 | ms/batch 28.57 | loss  4.88 | ppl   131.54\n",
            "| epoch  33 |   200/  217 batches | lr 2.50 | ms/batch 28.21 | loss  4.81 | ppl   122.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time:  7.01s | valid loss  6.25 | valid ppl   518.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   100/  217 batches | lr 1.25 | ms/batch 28.54 | loss  4.87 | ppl   129.87\n",
            "| epoch  34 |   200/  217 batches | lr 1.25 | ms/batch 28.25 | loss  4.79 | ppl   120.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time:  7.01s | valid loss  6.25 | valid ppl   516.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   100/  217 batches | lr 0.62 | ms/batch 28.54 | loss  4.86 | ppl   128.78\n",
            "| epoch  35 |   200/  217 batches | lr 0.62 | ms/batch 28.21 | loss  4.78 | ppl   118.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time:  7.01s | valid loss  6.25 | valid ppl   518.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   100/  217 batches | lr 0.31 | ms/batch 28.55 | loss  4.86 | ppl   128.52\n",
            "| epoch  36 |   200/  217 batches | lr 0.31 | ms/batch 28.20 | loss  4.77 | ppl   118.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time:  7.01s | valid loss  6.25 | valid ppl   519.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   100/  217 batches | lr 0.16 | ms/batch 28.53 | loss  4.85 | ppl   127.89\n",
            "| epoch  37 |   200/  217 batches | lr 0.16 | ms/batch 28.29 | loss  4.77 | ppl   118.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time:  7.02s | valid loss  6.25 | valid ppl   517.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   100/  217 batches | lr 0.08 | ms/batch 28.56 | loss  4.86 | ppl   128.57\n",
            "| epoch  38 |   200/  217 batches | lr 0.08 | ms/batch 28.23 | loss  4.78 | ppl   118.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time:  7.01s | valid loss  6.25 | valid ppl   518.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   100/  217 batches | lr 0.04 | ms/batch 28.56 | loss  4.85 | ppl   128.10\n",
            "| epoch  39 |   200/  217 batches | lr 0.04 | ms/batch 28.24 | loss  4.77 | ppl   118.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time:  7.02s | valid loss  6.25 | valid ppl   518.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   100/  217 batches | lr 0.02 | ms/batch 28.57 | loss  4.85 | ppl   128.03\n",
            "| epoch  40 |   200/  217 batches | lr 0.02 | ms/batch 28.25 | loss  4.77 | ppl   117.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time:  7.02s | valid loss  6.25 | valid ppl   518.00\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq_PkPQOGVcB"
      },
      "source": [
        "# Text Generation example\n",
        "\n",
        "Language Models are the basis of many tasks. They can be used for classification tasks as pretrained models that are transfered into learning new tasks by fine-tuning them on those down-stream tasks.\n",
        "And they can also be used for tasks that requires generating text. For example:\n",
        "* Text Summarization\n",
        "* Getting a response from a personalized Chat bots\n",
        "* Annotating an image\n",
        "\n",
        "Can you think of additional examples where text generation is needed?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the language model we've just trained to generate text.\n",
        "\n",
        "You can play with the parameters, such as `words_to_generate`, `temprature` to see how it affects the output."
      ],
      "metadata": {
        "id": "sWIn2HN5uj01"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "477FsXfZuDQ3",
        "outputId": "eb843073-11a3-4fad-990b-a3533f8a5e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.eval()\n",
        "\n",
        "log_interval = 100\n",
        "words_to_generate = 50\n",
        "temperature = 1. # higher temperature will increase diversity\n",
        "\n",
        "# generate random start\n",
        "input = torch.randint(10000, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "hidden = model.init_hidden(1)\n",
        "\n",
        "generated_word_ids = []\n",
        "\n",
        "with torch.no_grad():  # no tracking history\n",
        " for i in range(words_to_generate):\n",
        "    output, hidden = model(input, hidden)\n",
        "    word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "    input.fill_(word_idx)\n",
        "\n",
        "    generated_word_ids.append(word_idx.tolist())\n",
        "    # word = bpemb_en.decode_ids([word_idx.tolist()])\n",
        "    # print(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "    # if i % log_interval == 0:\n",
        "    #     print('| Generated {}/{} words'.format(i, words_to_generate))\n",
        "\n",
        "bpemb_en.decode_ids(generated_word_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'. sudden is, as a child-inateringched, and then, went into the warm g agreedply falling, which sm f was and theity rightust. the first ised into f nightly, monthly-out at'"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI9JwCE8GZcD"
      },
      "source": [
        "As discussed in class, the RNN/LSTM can be used in various tasks:\n",
        "\n",
        "it can be used for sequence2sequence, where the sequence size is either the same or different. Some examples include: \n",
        "* Translation\n",
        "* Tagging words as POS / SLR / NER\n",
        "* Encoding a document as a vector for classification\n",
        "\n",
        "etc."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train like a pro\n",
        "## DataLoader & pyTorch wrappers\n",
        "\n",
        "In real-world projects, we don't use 'batchify', but instead use the premade tools from Pytorch, such as the [DataLoader](https://pytorch.org/docs/stable/data.html).\n",
        "\n",
        "[Pytorch-ignite](https://pytorch.org/ignite/index.html) and [pytorch lightning](https://www.pytorchlightning.ai/) are two common libraries that are used to speed up development with Python.\n",
        "\n",
        "pyTorch Lightning organizes the code by wrapping the model into python classes, and separates the model from the data (and the data loading). It also has various of pre-defined and pre-trained models to quickly experiment and research.\n",
        "\n",
        "pyTorch Ignite offers a set of callbacks to be used during training.\n",
        "\n",
        "Both libraries have helper tools for validation metrics (RUC, accuracy, confusion matrix, etc.) as well as learning rate finder tools."
      ],
      "metadata": {
        "id": "ttMpR_z_un8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your turn:\n",
        "Rewrite the code above to be using pyTorch lightning.\n",
        "\n",
        "* This guide will help you converting the model into a Pytorch Lightning one:\n",
        "https://pytorch-lightning.readthedocs.io/en/stable/starter/converting.html\n",
        "\n",
        "* Use CrossEntropyLoss instad of NLL with Softmax (it's a combination of the two) https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
        "* Use the library learning-rate finder to decide the best learning rate for the training."
      ],
      "metadata": {
        "id": "MhOw_1bkvnIb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeEVfpmQDd3g"
      },
      "source": [
        "%%capture\n",
        "! pip install pytorch-lightning"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import pytorch_lightning as pl"
      ],
      "metadata": {
        "id": "n_ppY2gGvI1o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PLRNNModel(pl.LightningModule):\n",
        "  def __init__(self, \n",
        "               ninp=bpemb_en.vectors.shape[1], \n",
        "               noutp=bpemb_en.vocab_size, \n",
        "               nhid=256, \n",
        "               nlayers=2, \n",
        "               dropout=0.5, \n",
        "               tie_weights=False):\n",
        "    super().__init__()\n",
        "    # --------------------------\n",
        "    # Implement the RNN model here.\n",
        "    # Hint: This function should define the model layers\n",
        "    \n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "  \n",
        "  def forward(self, x, hidden):\n",
        "    # implement the forward pass here\n",
        "    pass\n",
        "\n",
        "  def training_step(self, batch, batch_idx):\n",
        "    # --------------------------\n",
        "    # Implement the training step here.\n",
        "    # Hint: This function should return the loss\n",
        "    pass\n",
        "\n",
        "  def validation_step(self, batch, batch_idx):\n",
        "    # --------------------------\n",
        "    # Implement the validation step here.\n",
        "    pass\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    # this time we will be using ADAM optimizer\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "    return optimizer\n",
        "\n",
        "  def repackage_hidden(self, h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(self.repackage_hidden(v).to(device) for v in h)\n"
      ],
      "metadata": {
        "id": "r-ggby_tvXxO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of the `batchify` function we created before, this time we will use pytorch DataLoader - which have already implemented all the functionality for us. We will combine it with `TensorDataset`, creating pairs of our text and the target."
      ],
      "metadata": {
        "id": "VttabUrWwX6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self,x, y):\n",
        "        self.x = torch.tensor(x, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "        self.len = self.x.shape[0]\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        return self.x[idx],self.y[idx]\n",
        "  \n",
        "    def __len__(self):\n",
        "        return self.len\n"
      ],
      "metadata": {
        "id": "W8sOLQ-NdN1q"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_ds = TextDataset(train_encoded_ids, train_encoded_ids[1:])\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size)\n",
        "\n",
        "val_ds = train_ds # Implement the correct validation dataset\n",
        "val_dl = train_dl # Implement the correct validation data loader"
      ],
      "metadata": {
        "id": "EOSyMJftdh17"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate that the data looks as you expect:\n",
        "next(iter(train_dl))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8u7-pPn5KD4",
        "outputId": "8eca52bc-c82d-452a-8219-a577939b93f3"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([1187, 9924, 2206, 9934,  101,  473,   56,   66, 7468, 5468,  386, 8281,\n",
              "          809, 3226, 9937, 9920, 2481,   64, 9976,  339,  233,   26,  176,   22,\n",
              "          106,  102, 9935, 9976, 9941, 9912, 9976, 9925]),\n",
              " tensor([9924, 2206, 9934,  101,  473,   56,   66, 7468, 5468,  386, 8281,  809,\n",
              "         3226, 9937, 9920, 2481,   64, 9976,  339,  233,   26,  176,   22,  106,\n",
              "          102, 9935, 9976, 9941, 9912, 9976, 9925,  437])]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init model\n",
        "rnn_model = PLRNNModel()\n",
        "\n",
        "# Initialize a trainer\n",
        "trainer = pl.Trainer(gpus=0, max_epochs=3)\n",
        "\n",
        "# Train the model ⚡\n",
        "trainer.fit(rnn_model, train_dl, val_dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "531c4031758c49ba92607befcdc8d312",
            "82939dcf9c1b48f5814a76293dea4eaa",
            "77780c380c7f4aed8a539d794e8cc1e5",
            "cd24062d23444afc81b8eecc98c103b6",
            "3356680e8be443158036ecf03511e310",
            "a33f7ac79cc24b8085ea68dade29b82a",
            "a91b2e1a7e094030bf72c4a32edea091",
            "82e901e49012475da63fbf558760d7a2",
            "bbd077c1cbc14080bc719f8f84b3a00a",
            "ca2477f0a59740a39e70c2293db11621",
            "3c0b752b2eb247ffb2b55b904fb97998",
            "97f1ad50a0174d3a895a2659d4ea00a5",
            "4154ae5ae9704e57b4594edf9ebdeb2f",
            "3bfae2b2aa9e49dbbbd3362b06bf6621",
            "663d692033524475821f1c7bb2c33bac",
            "6e229851ec8f46408c99526fe2218189",
            "c9ab56cdf8424538a4f02743b3445966",
            "b03d8f2a7efd443c951ebd6ade35be73",
            "c0705c356efb45e38b3fbe3523b8345b",
            "f47bb46d42284626a3218f5b8a72a510",
            "2f0847555060472cb4feaf334f6fa602",
            "2c4344e6fde34d98b315ff823811ec5b"
          ]
        },
        "id": "qiByHkLtwgRH",
        "outputId": "2842620f-6cad-47dc-eaf2-0606a850207c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | drop      | Dropout          | 0     \n",
            "1 | encoder   | Embedding        | 1.0 M \n",
            "2 | rnn       | LSTM             | 892 K \n",
            "3 | decoder   | Linear           | 2.6 M \n",
            "4 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "3.5 M     Trainable params\n",
            "1.0 M     Non-trainable params\n",
            "4.5 M     Total params\n",
            "17.852    Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "531c4031758c49ba92607befcdc8d312",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Validation sanity check: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97f1ad50a0174d3a895a2659d4ea00a5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start tensorboard.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs/"
      ],
      "metadata": {
        "id": "W8mhEBYuzMQW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}