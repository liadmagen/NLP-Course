{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "09_LM_LSTM Language Model & Word-parts.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liadmagen/NLP-Course/blob/master/09_LM_LSTM_Language_Model_%26_Word_parts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5zwGMzVD0pF"
      },
      "source": [
        "# RNN / BiLSTM and Word Vectors\n",
        "\n",
        "We'll load the frankenstein book, and convert it into semantic representation through word vectors.\n",
        "\n",
        "Then we will train a language model using LSTM on these vectors.\n",
        "\n",
        "After you run this notebook, please try changing the data-source from \"frankenstein.txt\" to \"dracula.txt\", and observe the result. \n",
        "\n",
        "### How are we going to do it?\n",
        "\n",
        "We will define our data, as such, that for every word we use as an input for the model (X = Wn), the next word would be the output (Y = Wn+1)\n",
        "\n",
        "The words in the output, Y, will be represented as a one-hot-vector. \n",
        "\n",
        "**Q: What is the size of this Vector?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NTgD9mVh-gi",
        "outputId": "1853370d-ee5a-4e0a-ae05-3b50a6ede76b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install bpemb"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bpemb\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bpemb) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from bpemb) (4.62.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb) (1.19.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from bpemb) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->bpemb) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb) (1.24.3)\n",
            "Installing collected packages: sentencepiece, bpemb\n",
            "Successfully installed bpemb-0.3.3 sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrUhaljjLeco"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import math\n",
        "import unicodedata\n",
        "import string\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, tensor\n",
        "\n",
        "from torchtext.data import get_tokenizer\n",
        "\n",
        "from bpemb import BPEmb"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYT3W-Rekse4"
      },
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSl2MJaLD-au"
      },
      "source": [
        "# Word Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spXr-wu8D8P2"
      },
      "source": [
        "Let's convert the text into vectors.\n",
        "\n",
        "We will use a package called [BPEmb](https://nlp.h-its.org/bpemb/) which encodes words to vectors by dividing these words to sub-words, pieces of words, made of characters which often appear together.\n",
        "\n",
        "Q: Remember what is the name of the Linguistic level that deals with letter-level? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgKbDV-5qAXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db32648-df31-45e9-8a0c-72561a0280f9"
      },
      "source": [
        "bpemb_en = BPEmb(lang=\"en\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 400869/400869 [00:00<00:00, 1237344.23B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.d100.w2v.bin.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3784656/3784656 [00:00<00:00, 4930574.00B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnWzITC8qCvw",
        "outputId": "9a402f3c-f302-4529-ed42-366d9426902f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bpemb_en.vectors.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaV2IRzjEClD"
      },
      "source": [
        "Let's create a function to load the corpus data (the books):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVl_hJ8yNktz"
      },
      "source": [
        "def get_file(filename = \"frankenstein.txt\"):\n",
        "  path = tf.keras.utils.get_file(\n",
        "      filename, origin=f\"https://raw.githubusercontent.com/liadmagen/NLP-Course/master/dataset/{filename}\"\n",
        "  )\n",
        "  with open(path, encoding=\"utf-8\") as f:\n",
        "      text = f.read() \n",
        "  text = text.replace(\"\\n\", \" \")        # Remove line-breaks & newlines\n",
        "  print(\"Corpus length:\", len(text))\n",
        "  return text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2iZ2Ol3EbU9"
      },
      "source": [
        "# RNN Model\n",
        "And this is the model itself. This is a very raw structure of it. \n",
        "\n",
        "Note: In 'real-ilfe' we're using helping frameworks such as [ignite](https://pytorch.org/ignite/) or [lightning](https://www.pytorchlightning.ai/). \n",
        "\n",
        "We bring it in this version here, for learning purposes only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a768GRY4TXVH"
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
        "\n",
        "    def __init__(self, ninp, noutp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "          ninp =  LSTM input size \n",
        "          noutp = size of the output (number of classes)\n",
        "          nhid = number of neurons in the hidden layer\n",
        "          nlayers = number of hidden layer\n",
        "          dropout = dropout rate\n",
        "          tie_weights = whether to use tie_weights (see note)\n",
        "        \"\"\"\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.noutp = noutp\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "        self.encoder = nn.Embedding.from_pretrained(tensor(bpemb_en.vectors))\n",
        "        \n",
        "        # self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity='relu', dropout=dropout)\n",
        "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
        "\n",
        "        self.decoder = nn.Linear(nhid, noutp)\n",
        "\n",
        "        # Optionally tie weights as in:\n",
        "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
        "        # https://arxiv.org/abs/1608.05859\n",
        "        # and\n",
        "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
        "        # https://arxiv.org/abs/1611.01462\n",
        "        if tie_weights:\n",
        "            if nhid != ninp:\n",
        "                raise ValueError('When using the tied flag, nhid must be equal to ninp (embedding size)')\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
        "        nn.init.zeros_(self.decoder.weight)\n",
        "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output)\n",
        "        decoded = decoded.view(-1, self.noutp)\n",
        "        return F.log_softmax(decoded, dim=1), hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters())\n",
        "        return (weight.new_zeros(self.nlayers, batch_size, self.nhid),\n",
        "                weight.new_zeros(self.nlayers, batch_size, self.nhid))\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsVkK0WRE4us"
      },
      "source": [
        "A helper class to convert the tokens into batches:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cc83Hwah0Hc"
      },
      "source": [
        "def batchify(data, batch_size):\n",
        "    # Work out how cleanly we can divide the dataset into batch_size parts.\n",
        "    nbatch = data.size(0) // batch_size\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * batch_size)\n",
        "    # Evenly divide the data across the batch_size batches.\n",
        "    data = data.view(batch_size, -1).t().contiguous()\n",
        "    return data.to(device)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFFTghmZE8JO"
      },
      "source": [
        "Let's load the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJq1xx_Lj813",
        "outputId": "f72f6e1f-0e32-4a0f-852c-183014e081f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_corpus = get_file('dracula.txt')\n",
        "val_corpus = get_file('frankenstein.txt')\n",
        "\n",
        "print(train_corpus[:300])\n",
        "print(val_corpus[:300])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/liadmagen/NLP-Course/master/dataset/dracula.txt\n",
            "860160/857524 [==============================] - 0s 0us/step\n",
            "868352/857524 [==============================] - 0s 0us/step\n",
            "Corpus length: 842159\n",
            "Downloading data from https://raw.githubusercontent.com/liadmagen/NLP-Course/master/dataset/frankenstein.txt\n",
            "434176/430265 [==============================] - 0s 0us/step\n",
            "442368/430265 [==============================] - 0s 0us/step\n",
            "Corpus length: 420726\n",
            "Dracula, by Bram Stoker  CHAPTER I  JONATHAN HARKER'S JOURNAL  (_Kept in shorthand._)   _3 May. Bistritz._--Left Munich at 8:35 P. M., on 1st May, arriving at Vienna early next morning; should have arrived at 6:46, but train was an hour late. Buda-Pesth seems a wonderful place, from the glimpse whic\n",
            "Frankenstein, or, the Modern Prometheus by Mary Wollstonecraft (Godwin) Shelley  Letter 1  _To Mrs. Saville, England._   St. Petersburgh, Dec. 11th, 17—.   You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rA2YyF8E9-5"
      },
      "source": [
        "# Semantic representation + word-parts\n",
        "\n",
        "And convert it into vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC4A4ygqiYDd"
      },
      "source": [
        "train_encoded_text = bpemb_en.encode(train_corpus)\n",
        "train_encoded_ids = bpemb_en.encode_ids(train_corpus)\n",
        "\n",
        "val_encoded_text = bpemb_en.encode(val_corpus)\n",
        "val_encoded_ids = bpemb_en.encode_ids(val_corpus)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXmw96xvFEzL"
      },
      "source": [
        "Let's check the result of encoded_text (we'll get to encoded_ids in a moment).\n",
        "\n",
        "Notice that every word is now broken to pieces. \n",
        "\n",
        "A **'_'** mark in the beginning of a token, represents a beginning of a new word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oki7xTl5FDyT",
        "outputId": "624f34d1-7a64-49bd-98df-467a7c23123e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_encoded_text[:50]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁dra',\n",
              " 'c',\n",
              " 'ula',\n",
              " ',',\n",
              " '▁by',\n",
              " '▁br',\n",
              " 'am',\n",
              " '▁st',\n",
              " 'oker',\n",
              " '▁chapter',\n",
              " '▁i',\n",
              " '▁jonathan',\n",
              " '▁har',\n",
              " 'ker',\n",
              " \"'\",\n",
              " 's',\n",
              " '▁journal',\n",
              " '▁(',\n",
              " '_',\n",
              " 'ke',\n",
              " 'pt',\n",
              " '▁in',\n",
              " '▁sh',\n",
              " 'or',\n",
              " 'th',\n",
              " 'and',\n",
              " '.',\n",
              " '_',\n",
              " ')',\n",
              " '▁',\n",
              " '_',\n",
              " '0',\n",
              " '▁may',\n",
              " '.',\n",
              " '▁b',\n",
              " 'ist',\n",
              " 'rit',\n",
              " 'z',\n",
              " '.',\n",
              " '_',\n",
              " '-',\n",
              " '-',\n",
              " 'left',\n",
              " '▁mun',\n",
              " 'ich',\n",
              " '▁at',\n",
              " '▁0:00',\n",
              " '▁p',\n",
              " '.',\n",
              " '▁m']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkpMGwmYFR8E"
      },
      "source": [
        "This method is called word-parts. \n",
        "\n",
        "Instead of converting a whole word (word2vec, gloVe), or a character (FastText), this method converts slices of text, a combination of characters, together.\n",
        "\n",
        "It does so by finding the most common combinations, most frequent combinations, of characters in a very big corpus. \n",
        "\n",
        "The result is having a vocabulary which is WAY smaller than all-the-words (how big would that be?) bug bigger than all the characters:\n",
        "\n",
        "**character-based << word-piece based << word-based**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21-E5O3tF9HK"
      },
      "source": [
        "# Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGD8BiR7oT11"
      },
      "source": [
        "batch_size = 32\n",
        "eval_batch_size = 32\n",
        "\n",
        "vocab_size = bpemb_en.vocab_size\n",
        "embsize = bpemb_en.vectors.shape[1]\n",
        "nhidden = 256\n",
        "nlayers = 2"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAdLH-8YuILA"
      },
      "source": [
        "model = RNNModel(embsize, vocab_size, nhidden, nlayers).to(device)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2g7-0EkqYZh"
      },
      "source": [
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ9zbXUwGBXd"
      },
      "source": [
        "# Division to train/validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyj2AlPwuLui"
      },
      "source": [
        "train_enc_ids = torch.tensor(train_encoded_ids).type(torch.int64)\n",
        "train_data = batchify(train_enc_ids, batch_size)\n",
        "\n",
        "val_enc_ids = torch.tensor(val_encoded_ids).type(torch.int64)\n",
        "val_data = batchify(val_enc_ids, batch_size)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaDDZuY7qyRH"
      },
      "source": [
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj9TGJMkypM9"
      },
      "source": [
        "def get_batch(source, i):\n",
        "    seq_len = min(batch_size, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdvF1m90GGtJ"
      },
      "source": [
        "# Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLQmqGbuq39-"
      },
      "source": [
        "def train(train_data, log_interval = 100):\n",
        "    # Turn on training mode - which enables dropout.\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0.\n",
        "\n",
        "    start_time = time.time()\n",
        "    ntokens = len(train_data)\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, batch_size)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        model.zero_grad()\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
        "        for p in model.parameters():\n",
        "          if p.grad is not None:\n",
        "            p.data.add_(p.grad, alpha=-lr)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // batch_size, lr,\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REFFlXWO0MbI"
      },
      "source": [
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(data_source)\n",
        "\n",
        "    hidden = model.init_hidden(eval_batch_size)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, batch_size):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data, hidden)\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvhCcxsTGPk0"
      },
      "source": [
        "# Training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSd0s9jUuA3O",
        "outputId": "0d470161-74bc-47ff-c348-30d0ccef46f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Loop over epochs.\n",
        "lr = 20\n",
        "best_val_loss = None\n",
        "epochs = 40\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_data)\n",
        "    val_loss = evaluate(val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "            'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                        val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if not best_val_loss or val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "    else:\n",
        "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "        lr /= 2.0"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   100/  217 batches | lr 20.00 | ms/batch 31.28 | loss  7.23 | ppl  1386.43\n",
            "| epoch   1 |   200/  217 batches | lr 20.00 | ms/batch 28.21 | loss  6.63 | ppl   754.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  7.27s | valid loss  7.04 | valid ppl  1140.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   100/  217 batches | lr 20.00 | ms/batch 28.46 | loss  6.58 | ppl   722.67\n",
            "| epoch   2 |   200/  217 batches | lr 20.00 | ms/batch 28.19 | loss  6.46 | ppl   636.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  6.99s | valid loss  6.94 | valid ppl  1032.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   100/  217 batches | lr 20.00 | ms/batch 28.50 | loss  6.46 | ppl   641.77\n",
            "| epoch   3 |   200/  217 batches | lr 20.00 | ms/batch 28.19 | loss  6.34 | ppl   567.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time:  7.00s | valid loss  6.94 | valid ppl  1030.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   100/  217 batches | lr 20.00 | ms/batch 28.52 | loss  6.33 | ppl   561.91\n",
            "| epoch   4 |   200/  217 batches | lr 20.00 | ms/batch 28.26 | loss  6.16 | ppl   473.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time:  7.01s | valid loss  6.77 | valid ppl   868.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   100/  217 batches | lr 20.00 | ms/batch 28.53 | loss  6.15 | ppl   469.02\n",
            "| epoch   5 |   200/  217 batches | lr 20.00 | ms/batch 28.19 | loss  6.00 | ppl   403.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time:  7.00s | valid loss  6.66 | valid ppl   777.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   100/  217 batches | lr 20.00 | ms/batch 28.55 | loss  6.00 | ppl   404.26\n",
            "| epoch   6 |   200/  217 batches | lr 20.00 | ms/batch 28.21 | loss  5.88 | ppl   356.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time:  7.01s | valid loss  6.65 | valid ppl   770.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   100/  217 batches | lr 20.00 | ms/batch 28.60 | loss  5.89 | ppl   363.04\n",
            "| epoch   7 |   200/  217 batches | lr 20.00 | ms/batch 28.24 | loss  5.77 | ppl   319.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time:  7.02s | valid loss  6.55 | valid ppl   698.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   100/  217 batches | lr 20.00 | ms/batch 28.60 | loss  5.78 | ppl   325.31\n",
            "| epoch   8 |   200/  217 batches | lr 20.00 | ms/batch 28.24 | loss  5.69 | ppl   295.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time:  7.02s | valid loss  6.50 | valid ppl   664.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   100/  217 batches | lr 20.00 | ms/batch 28.60 | loss  5.71 | ppl   303.10\n",
            "| epoch   9 |   200/  217 batches | lr 20.00 | ms/batch 28.26 | loss  5.61 | ppl   273.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time:  7.02s | valid loss  6.48 | valid ppl   652.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   100/  217 batches | lr 20.00 | ms/batch 28.53 | loss  5.64 | ppl   281.35\n",
            "| epoch  10 |   200/  217 batches | lr 20.00 | ms/batch 28.23 | loss  5.55 | ppl   256.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time:  7.01s | valid loss  6.47 | valid ppl   642.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   100/  217 batches | lr 20.00 | ms/batch 28.55 | loss  5.58 | ppl   265.20\n",
            "| epoch  11 |   200/  217 batches | lr 20.00 | ms/batch 28.20 | loss  5.49 | ppl   243.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time:  7.01s | valid loss  6.44 | valid ppl   625.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   100/  217 batches | lr 20.00 | ms/batch 28.53 | loss  5.52 | ppl   249.59\n",
            "| epoch  12 |   200/  217 batches | lr 20.00 | ms/batch 28.23 | loss  5.43 | ppl   228.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time:  7.01s | valid loss  6.40 | valid ppl   602.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   100/  217 batches | lr 20.00 | ms/batch 28.56 | loss  5.47 | ppl   237.03\n",
            "| epoch  13 |   200/  217 batches | lr 20.00 | ms/batch 28.29 | loss  5.39 | ppl   218.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time:  7.02s | valid loss  6.38 | valid ppl   592.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   100/  217 batches | lr 20.00 | ms/batch 28.61 | loss  5.42 | ppl   226.26\n",
            "| epoch  14 |   200/  217 batches | lr 20.00 | ms/batch 28.19 | loss  5.34 | ppl   208.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time:  7.02s | valid loss  6.38 | valid ppl   588.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   100/  217 batches | lr 20.00 | ms/batch 28.58 | loss  5.38 | ppl   216.80\n",
            "| epoch  15 |   200/  217 batches | lr 20.00 | ms/batch 28.29 | loss  5.30 | ppl   199.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time:  7.03s | valid loss  6.36 | valid ppl   575.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   100/  217 batches | lr 20.00 | ms/batch 28.59 | loss  5.34 | ppl   207.48\n",
            "| epoch  16 |   200/  217 batches | lr 20.00 | ms/batch 28.22 | loss  5.26 | ppl   192.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time:  7.02s | valid loss  6.33 | valid ppl   562.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   100/  217 batches | lr 20.00 | ms/batch 28.55 | loss  5.30 | ppl   200.08\n",
            "| epoch  17 |   200/  217 batches | lr 20.00 | ms/batch 28.23 | loss  5.22 | ppl   185.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time:  7.01s | valid loss  6.33 | valid ppl   562.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   100/  217 batches | lr 20.00 | ms/batch 28.51 | loss  5.26 | ppl   192.53\n",
            "| epoch  18 |   200/  217 batches | lr 20.00 | ms/batch 28.23 | loss  5.19 | ppl   179.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time:  7.01s | valid loss  6.31 | valid ppl   549.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   100/  217 batches | lr 20.00 | ms/batch 28.59 | loss  5.23 | ppl   186.02\n",
            "| epoch  19 |   200/  217 batches | lr 20.00 | ms/batch 28.22 | loss  5.15 | ppl   172.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time:  7.02s | valid loss  6.30 | valid ppl   546.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   100/  217 batches | lr 20.00 | ms/batch 28.52 | loss  5.20 | ppl   181.34\n",
            "| epoch  20 |   200/  217 batches | lr 20.00 | ms/batch 28.22 | loss  5.12 | ppl   167.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time:  7.01s | valid loss  6.31 | valid ppl   550.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   100/  217 batches | lr 10.00 | ms/batch 28.53 | loss  5.12 | ppl   168.01\n",
            "| epoch  21 |   200/  217 batches | lr 10.00 | ms/batch 28.24 | loss  5.02 | ppl   152.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time:  7.01s | valid loss  6.29 | valid ppl   536.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   100/  217 batches | lr 10.00 | ms/batch 28.56 | loss  5.08 | ppl   160.64\n",
            "| epoch  22 |   200/  217 batches | lr 10.00 | ms/batch 28.22 | loss  5.01 | ppl   149.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time:  7.01s | valid loss  6.27 | valid ppl   530.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   100/  217 batches | lr 10.00 | ms/batch 28.56 | loss  5.06 | ppl   157.12\n",
            "| epoch  23 |   200/  217 batches | lr 10.00 | ms/batch 28.28 | loss  4.98 | ppl   145.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time:  7.02s | valid loss  6.27 | valid ppl   528.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   100/  217 batches | lr 10.00 | ms/batch 28.52 | loss  5.05 | ppl   155.31\n",
            "| epoch  24 |   200/  217 batches | lr 10.00 | ms/batch 28.21 | loss  4.96 | ppl   143.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time:  7.00s | valid loss  6.27 | valid ppl   527.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   100/  217 batches | lr 10.00 | ms/batch 28.49 | loss  5.02 | ppl   151.97\n",
            "| epoch  25 |   200/  217 batches | lr 10.00 | ms/batch 28.22 | loss  4.95 | ppl   141.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time:  7.01s | valid loss  6.26 | valid ppl   525.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   100/  217 batches | lr 10.00 | ms/batch 28.53 | loss  5.01 | ppl   149.96\n",
            "| epoch  26 |   200/  217 batches | lr 10.00 | ms/batch 28.22 | loss  4.93 | ppl   138.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time:  7.01s | valid loss  6.26 | valid ppl   521.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   100/  217 batches | lr 10.00 | ms/batch 28.53 | loss  4.99 | ppl   147.25\n",
            "| epoch  27 |   200/  217 batches | lr 10.00 | ms/batch 28.21 | loss  4.92 | ppl   137.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time:  7.01s | valid loss  6.25 | valid ppl   519.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   100/  217 batches | lr 10.00 | ms/batch 28.63 | loss  4.97 | ppl   144.19\n",
            "| epoch  28 |   200/  217 batches | lr 10.00 | ms/batch 28.26 | loss  4.90 | ppl   134.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time:  7.03s | valid loss  6.27 | valid ppl   527.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   100/  217 batches | lr 5.00 | ms/batch 28.44 | loss  4.94 | ppl   139.14\n",
            "| epoch  29 |   200/  217 batches | lr 5.00 | ms/batch 28.23 | loss  4.85 | ppl   128.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time:  7.01s | valid loss  6.25 | valid ppl   519.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   100/  217 batches | lr 2.50 | ms/batch 28.53 | loss  4.91 | ppl   135.45\n",
            "| epoch  30 |   200/  217 batches | lr 2.50 | ms/batch 28.26 | loss  4.82 | ppl   124.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time:  7.01s | valid loss  6.25 | valid ppl   518.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   100/  217 batches | lr 2.50 | ms/batch 28.57 | loss  4.89 | ppl   133.26\n",
            "| epoch  31 |   200/  217 batches | lr 2.50 | ms/batch 28.26 | loss  4.82 | ppl   123.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time:  7.02s | valid loss  6.25 | valid ppl   517.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   100/  217 batches | lr 2.50 | ms/batch 28.55 | loss  4.88 | ppl   132.28\n",
            "| epoch  32 |   200/  217 batches | lr 2.50 | ms/batch 28.21 | loss  4.81 | ppl   122.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time:  7.01s | valid loss  6.24 | valid ppl   515.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   100/  217 batches | lr 2.50 | ms/batch 28.57 | loss  4.88 | ppl   131.54\n",
            "| epoch  33 |   200/  217 batches | lr 2.50 | ms/batch 28.21 | loss  4.81 | ppl   122.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time:  7.01s | valid loss  6.25 | valid ppl   518.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   100/  217 batches | lr 1.25 | ms/batch 28.54 | loss  4.87 | ppl   129.87\n",
            "| epoch  34 |   200/  217 batches | lr 1.25 | ms/batch 28.25 | loss  4.79 | ppl   120.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time:  7.01s | valid loss  6.25 | valid ppl   516.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   100/  217 batches | lr 0.62 | ms/batch 28.54 | loss  4.86 | ppl   128.78\n",
            "| epoch  35 |   200/  217 batches | lr 0.62 | ms/batch 28.21 | loss  4.78 | ppl   118.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time:  7.01s | valid loss  6.25 | valid ppl   518.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   100/  217 batches | lr 0.31 | ms/batch 28.55 | loss  4.86 | ppl   128.52\n",
            "| epoch  36 |   200/  217 batches | lr 0.31 | ms/batch 28.20 | loss  4.77 | ppl   118.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time:  7.01s | valid loss  6.25 | valid ppl   519.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   100/  217 batches | lr 0.16 | ms/batch 28.53 | loss  4.85 | ppl   127.89\n",
            "| epoch  37 |   200/  217 batches | lr 0.16 | ms/batch 28.29 | loss  4.77 | ppl   118.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time:  7.02s | valid loss  6.25 | valid ppl   517.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   100/  217 batches | lr 0.08 | ms/batch 28.56 | loss  4.86 | ppl   128.57\n",
            "| epoch  38 |   200/  217 batches | lr 0.08 | ms/batch 28.23 | loss  4.78 | ppl   118.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time:  7.01s | valid loss  6.25 | valid ppl   518.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   100/  217 batches | lr 0.04 | ms/batch 28.56 | loss  4.85 | ppl   128.10\n",
            "| epoch  39 |   200/  217 batches | lr 0.04 | ms/batch 28.24 | loss  4.77 | ppl   118.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time:  7.02s | valid loss  6.25 | valid ppl   518.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   100/  217 batches | lr 0.02 | ms/batch 28.57 | loss  4.85 | ppl   128.03\n",
            "| epoch  40 |   200/  217 batches | lr 0.02 | ms/batch 28.25 | loss  4.77 | ppl   117.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time:  7.02s | valid loss  6.25 | valid ppl   518.00\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq_PkPQOGVcB"
      },
      "source": [
        "# Text Generation example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "477FsXfZuDQ3",
        "outputId": "eb843073-11a3-4fad-990b-a3533f8a5e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.eval()\n",
        "\n",
        "log_interval = 100\n",
        "words_to_generate = 50\n",
        "temperature = 1. # higher temperature will increase diversity\n",
        "\n",
        "# generate random start\n",
        "input = torch.randint(10000, (1, 1), dtype=torch.long).to(device)\n",
        "\n",
        "hidden = model.init_hidden(1)\n",
        "\n",
        "generated_word_ids = []\n",
        "\n",
        "with torch.no_grad():  # no tracking history\n",
        " for i in range(words_to_generate):\n",
        "    output, hidden = model(input, hidden)\n",
        "    word_weights = output.squeeze().div(temperature).exp().cpu()\n",
        "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "    input.fill_(word_idx)\n",
        "\n",
        "    generated_word_ids.append(word_idx.tolist())\n",
        "    # word = bpemb_en.decode_ids([word_idx.tolist()])\n",
        "    # print(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "    # if i % log_interval == 0:\n",
        "    #     print('| Generated {}/{} words'.format(i, words_to_generate))\n",
        "\n",
        "bpemb_en.decode_ids(generated_word_ids)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'. sudden is, as a child-inateringched, and then, went into the warm g agreedply falling, which sm f was and theity rightust. the first ised into f nightly, monthly-out at'"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI9JwCE8GZcD"
      },
      "source": [
        "As discussed in class, the RNN/LSTM can be used to many various task:\n",
        "\n",
        "it can be used for sequence2sequence, where the sequence size is the same or different: \n",
        "* Translation\n",
        "* Tagging words as POS / SLR / NER\n",
        "* Encoding a document as a vector for classification\n",
        "\n",
        "etc."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader & pyTorch wrappers\n",
        "\n",
        "In real-world projects, we don't use 'batchify', but instead use the premade tools from Pytorch, such as the [dataloader](https://pytorch.org/docs/stable/data.html).\n",
        "\n",
        "[Pytorch-ignite](https://pytorch.org/ignite/index.html) and [pytorch lightning](https://www.pytorchlightning.ai/) are two common libraries that are used to speed up development with Python.\n",
        "\n",
        "pyTorch Lightning organizes the code by wrapping the model into python classes, and separates the model from the data (and the data loading). It also has various of pre-defined and pre-trained models to quickly experiment and research.\n",
        "\n",
        "pyTorch Ignite offers a set of callbacks to be used during training.\n",
        "\n",
        "Both libraries have helper tools for validation metrics (RUC, accuracy, confusion matrix, etc.) as well as learning rate finder tools."
      ],
      "metadata": {
        "id": "ttMpR_z_un8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your turn:\n",
        "Rewrite the code above to be using either pytoch-ignite or lightning, to your choice.\n",
        "\n",
        "Use the library learning-rate finder to decide the best learning rate for the training."
      ],
      "metadata": {
        "id": "MhOw_1bkvnIb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeEVfpmQDd3g"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}