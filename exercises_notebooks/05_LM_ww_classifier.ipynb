{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "05_LM_ww_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "95f6b111c74d4602aaeb4059f9f25774": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e035ffed832c4b3f9f439b7a126ced0c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_578610b1b5cf46ef828278aa1f3a1273",
              "IPY_MODEL_9bae9b2cba054689b254b45a89cd45e3",
              "IPY_MODEL_261801c8e972468fa4b1e1e1660cd7db"
            ]
          }
        },
        "e035ffed832c4b3f9f439b7a126ced0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "578610b1b5cf46ef828278aa1f3a1273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_16b730af931f476db36ca3498c9bd6d7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c48114ad9c3e45538e36730a828e31cf"
          }
        },
        "9bae9b2cba054689b254b45a89cd45e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c7822ebcd37b4aa8a29f5fad505d1806",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 10000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 10000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9edd66f814d94a3cb5e382f7a4cf02a8"
          }
        },
        "261801c8e972468fa4b1e1e1660cd7db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_620d3464579249e698043cc5d57f098a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 10000/10000 [00:14&lt;00:00, 726.53it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_027beeea2a4a49d58d5d26eedf6f9959"
          }
        },
        "16b730af931f476db36ca3498c9bd6d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c48114ad9c3e45538e36730a828e31cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7822ebcd37b4aa8a29f5fad505d1806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9edd66f814d94a3cb5e382f7a4cf02a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "620d3464579249e698043cc5d57f098a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "027beeea2a4a49d58d5d26eedf6f9959": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liadmagen/NLP-Course/blob/master/exercises_notebooks/05_LM_ww_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-3IkYpRnJbE"
      },
      "source": [
        "## Word Window Classification - introduction to pyTorch\n",
        "\n",
        "### pyTorch Exploration\n",
        "\n",
        "### Originally written by: Matthew Lamm\n",
        "\n",
        "### Adapted by: Liad Magen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGnaHAFUnJbF"
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import pprint\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "pp = pprint.PrettyPrinter()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWgE9eMsnJbL"
      },
      "source": [
        "## Our Data\n",
        "\n",
        "The task at hand is to assign a label of 1 to words in a sentence that correspond with a LOCATION, and a label of 0 to everything else. \n",
        "\n",
        "In this simplified example, we only ever see spans of length 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6Aqv2_RnJbM"
      },
      "source": [
        "# Training data:\n",
        "\n",
        "train_sents = [s.lower().split() for s in [\"we 'll always have Paris\",\n",
        "                                           \"I live in Germany\",\n",
        "                                           \"He comes from Denmark\",\n",
        "                                           \"The capital of Denmark is Copenhagen\"]]\n",
        "train_labels = [[0, 0, 0, 0, 1],\n",
        "                [0, 0, 0, 1],\n",
        "                [0, 0, 0, 1],\n",
        "                [0, 0, 0, 1, 0, 1]]\n",
        "\n",
        "# ensure that X match Y\n",
        "assert all([len(train_sents[i]) == len(train_labels[i]) for i in range(len(train_sents))])\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl9nTHCPnJbP"
      },
      "source": [
        "# Testing data\n",
        "\n",
        "test_sents = [s.lower().split() for s in [\"She comes from Paris\"]]\n",
        "test_labels = [[0, 0, 0, 1]]\n",
        "\n",
        "assert all([len(test_sents[i]) == len(test_labels[i]) for i in range(len(test_sents))])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai4LzzF-nJbS"
      },
      "source": [
        "## Creating a dataset of batched tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szxtRa7GnJbT"
      },
      "source": [
        "PyTorch (like other deep learning frameworks) is optimized to work on __tensors__, which can be thought of as a generalization of vectors and matrices with arbitrarily large rank.\n",
        "\n",
        "Here we'll go over how to translate data to a list of vocabulary indices, and how to construct *batch tensors* out of the data for easy input to our model. \n",
        "\n",
        "We'll use the *torch.utils.data.DataLoader* object handle ease of batching and iteration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J547I2uQnJbT"
      },
      "source": [
        "### Converting tokenized sentence lists to vocabulary indices.\n",
        "\n",
        "Let's assume we have the following vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2Xtdv6hnJbU"
      },
      "source": [
        "id_2_word = [\"<pad>\", \"<unk>\", \"we\", \"always\", \"have\", \"paris\",\n",
        "              \"i\", \"live\", \"in\", \"germany\",\n",
        "              \"he\", \"comes\", \"from\", \"denmark\",\n",
        "              \"the\", \"of\", \"is\", \"copenhagen\"]\n",
        "word_2_id = {w:i for i,w in enumerate(id_2_word)}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mxyldFnnJbX",
        "outputId": "3a0eaa32-697a-4742-e541-afe621ebf612",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "instance = train_sents[0]\n",
        "print(instance)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['we', \"'ll\", 'always', 'have', 'paris']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9suW0ZWnJbb"
      },
      "source": [
        "def convert_tokens_to_inds(sentence, word_2_id):\n",
        "    return [word_2_id.get(t, word_2_id[\"<unk>\"]) for t in sentence]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCgVhobvnJbe",
        "outputId": "d08f6d3e-3b62-413d-ae04-fd0a0fdb4b7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "token_inds = convert_tokens_to_inds(instance, word_2_id)\n",
        "pp.pprint(token_inds)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 1, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBu-QV2knJbh"
      },
      "source": [
        "Let's convince ourselves that worked:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCM96PSbnJbh",
        "outputId": "2e959a4d-ab8d-4cfb-b427-235b677aca29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print([id_2_word[tok_idx] for tok_idx in token_inds])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['we', '<unk>', 'always', 'have', 'paris']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh1e4vG9CrZy"
      },
      "source": [
        "As discussed in the class, due to the variability of the language, we will never be able to create a closed-list vocabulary, which contains all the possible words.\n",
        "\n",
        "Therefore, during the pre-processing, we transform unseen words to special pre-defined tokens in our vocabulary, such as **\\<unk\\>** which represents an **unknown** vocabulary word.\n",
        "\n",
        "Similar examples include replacing numbers with **\\<num\\>**, marking capitalized letters with **\\<up\\>** etc. \n",
        "\n",
        "#### Note:\n",
        "It is important to emphasize the these formats are framework-dependant. \n",
        "\n",
        "For example, in BERT and fastai, these tokens are marked as **xxunk**.\n",
        "\n",
        "Read more about it here:\n",
        "https://fastai1.fast.ai/text.transform.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwdDFWPPnJbk"
      },
      "source": [
        "### Padding for windows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JCJZkPLnJbk"
      },
      "source": [
        "In the word window classifier, we want to get for each word in the sentence the +/- *n* words window around that given word, where *0 <= n < len(sentence)*.\n",
        "\n",
        "**Special case: Words in the beginning / end of a sentence**\n",
        "\n",
        "Notice that we may get stuck with words in the beginning/end of the sentence - there's no room for that window.\n",
        "\n",
        "In order for such windows to be defined for words at the beginning and ends of the sentence, we need to insert padding around the sentence before converting to indices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfyB3U7KnJbl"
      },
      "source": [
        "def pad_sentence_for_window(sentence, window_size, pad_token=\"<pad>\"):\n",
        "    return [pad_token]*window_size + sentence + [pad_token]*window_size "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UFodsg-nJbo",
        "outputId": "a7dd0c8a-83b9-4fed-cfc9-4c0f2e1bb59b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## window_size represents the number of words to each side of a given word\n",
        "window_size = 2\n",
        "\n",
        "instance = pad_sentence_for_window(train_sents[0], window_size)\n",
        "print(instance)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>', '<pad>', 'we', \"'ll\", 'always', 'have', 'paris', '<pad>', '<pad>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH1rvJxnnJbq"
      },
      "source": [
        "Let's make sure this works with our vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "s7BTedlJnJbt",
        "outputId": "2e1822b7-30ae-4e4b-c61c-3c311fa786f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for sent in train_sents:\n",
        "    tok_idxs = convert_tokens_to_inds(pad_sentence_for_window(sent, window_size), word_2_id)\n",
        "    print([id_2_word[idx] for idx in tok_idxs])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<pad>', '<pad>', 'we', '<unk>', 'always', 'have', 'paris', '<pad>', '<pad>']\n",
            "['<pad>', '<pad>', 'i', 'live', 'in', 'germany', '<pad>', '<pad>']\n",
            "['<pad>', '<pad>', 'he', 'comes', 'from', 'denmark', '<pad>', '<pad>']\n",
            "['<pad>', '<pad>', 'the', '<unk>', 'of', 'denmark', 'is', 'copenhagen', '<pad>', '<pad>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88QMwyiQnJb5"
      },
      "source": [
        "### Batching sentences together with a DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKxKCHw4nJb7"
      },
      "source": [
        "When we train our model, we rarely update with respect to a single training instance at a time, because a single instance provides a very noisy estimate of the global loss's gradient. We instead construct small *batches*, a collection of several samples of the training data, and update parameters for each batch.\n",
        "\n",
        "As you may remember from the Deep Learning training, using batches - these grouped training data samples - helps the model to learn quicker. The learnt parameters are updated based on the average loss, which is calculated from all the batch's samples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmJK0wNuJNmQ"
      },
      "source": [
        "---\n",
        "*Test yourself*: Can you explain how using batches help the model to converge faster, compared to a providing a single sample in every training iteration?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifN1R-z1nJb8"
      },
      "source": [
        "Given some batch size, we want to construct batch tensors out of the word index lists we've just created with our vocab.\n",
        "\n",
        "For each length B list of inputs, we'll have to:\n",
        "\n",
        "    (1) Add window padding to sentences in the batch like we just saw.\n",
        "    (2) Add additional padding so that each sentence in the batch is the same length.\n",
        "    (3) Make sure our labels are in the desired format.\n",
        "\n",
        "At the level of the dataest we want:\n",
        "\n",
        "    (4) Easy shuffling, because shuffling from one training epoch to the next gets rid of \n",
        "        pathological batches that are tough to learn from.\n",
        "    (5) Making sure we shuffle inputs and their labels together!\n",
        "    \n",
        "PyTorch provides us with an object *torch.utils.data.DataLoader* that gets us (4) and (5). All that's required of us is to specify a *collate_fn* that tells it how to do (1), (2), and (3). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll0jWyEenJb-",
        "outputId": "c741c41e-4e2c-4947-a917-3b147f77233a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_labels_tensor = torch.LongTensor(train_labels[0])\n",
        "pp.pprint((\"raw train label instance\", train_labels_tensor))\n",
        "print(train_labels_tensor.size())\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('raw train label instance', tensor([0, 0, 0, 0, 1]))\n",
            "torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiTP8vkYYAqK",
        "outputId": "8e5a4b64-2984-4d95-b3ff-64b556552d7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pp.pprint((\"one-hot labels\", nn.functional.one_hot(train_labels_tensor).transpose(0, 1)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('one-hot labels', tensor([[1, 1, 1, 1, 0],\n",
            "        [0, 0, 0, 0, 1]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfeIBnWonJcP"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from functools import partial"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J85Bc__cnJcS"
      },
      "source": [
        "def my_collate(data, window_size, word_2_id):\n",
        "    \"\"\"\n",
        "    For some chunk of sentences and labels\n",
        "        -add winow padding\n",
        "        -pad for lengths using pad_sequence\n",
        "        -convert our labels to one-hots\n",
        "        -return padded inputs, one-hot labels, and lengths\n",
        "    \"\"\"\n",
        "    \n",
        "    x_s, y_s = zip(*data)\n",
        "\n",
        "    # deal with input sentences as we've seen\n",
        "    window_padded = [convert_tokens_to_inds(pad_sentence_for_window(sentence, window_size), word_2_id)\n",
        "                                                                                  for sentence in x_s]\n",
        "    # append zeros to each list of token ids in batch so that they are all the same length\n",
        "    padded = nn.utils.rnn.pad_sequence([torch.LongTensor(t) for t in window_padded], batch_first=True)\n",
        "    \n",
        "    # convert labels to one-hots\n",
        "    labels = []\n",
        "    lengths = []\n",
        "    for y in y_s:\n",
        "        lengths.append(len(y))\n",
        "        y_long = torch.LongTensor(y)\n",
        "        label = nn.functional.one_hot(y_long)\n",
        "        labels.append(label)\n",
        "\n",
        "    padded_labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "    \n",
        "    return padded.long(), padded_labels, torch.LongTensor(lengths)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NR5B2s6nJcV"
      },
      "source": [
        "# Shuffle True is good practice for train loaders.\n",
        "# Use functools.partial to construct a partially populated collate function\n",
        "example_loader = DataLoader(list(zip(train_sents, train_labels)), \n",
        "                                      batch_size=2, \n",
        "                                      shuffle=True, \n",
        "                                      collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BQ7lOnhnJcX",
        "outputId": "bf5b8f59-927c-4ee7-fab7-238a66a7b9c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for batched_input, batched_labels, batch_lengths in example_loader:\n",
        "    pp.pprint((\"inputs\", batched_input, batched_input.size()))\n",
        "    pp.pprint((\"labels\", batched_labels, batched_labels.size()))\n",
        "    pp.pprint(batch_lengths)\n",
        "    break"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('inputs',\n",
            " tensor([[ 0,  0, 14,  1, 15, 13, 16, 17,  0,  0],\n",
            "        [ 0,  0,  6,  7,  8,  9,  0,  0,  0,  0]]),\n",
            " torch.Size([2, 10]))\n",
            "('labels',\n",
            " tensor([[[1, 0],\n",
            "         [1, 0],\n",
            "         [1, 0],\n",
            "         [0, 1],\n",
            "         [1, 0],\n",
            "         [0, 1]],\n",
            "\n",
            "        [[1, 0],\n",
            "         [1, 0],\n",
            "         [1, 0],\n",
            "         [0, 1],\n",
            "         [0, 0],\n",
            "         [0, 0]]]),\n",
            " torch.Size([2, 6, 2]))\n",
            "tensor([6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k0mqI_unJca"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "### Thinking through vectorization of word windows.\n",
        "Before we go ahead and build our model, let's think about the first thing it needs to do to its inputs.\n",
        "\n",
        "We're passed batches of sentences. For each sentence *i* in the batch, for each word j in the sentence, we want to construct a single tensor out of the embeddings surrounding word j in the +/- n window.\n",
        "\n",
        "Thus, the first thing we're going to need a (B, L, 2N+1) tensor of token indices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7db_pQ1nJca"
      },
      "source": [
        "A *terrible* but nevertheless informative *iterative* solution looks something like the following, where we iterate through batch elements in our (dummy), iterating non-padded word positions in those, and for each non-padded word position, construct a window:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsCMqHEknJca",
        "outputId": "d214cf48-f166-44c7-835b-1bef84500eb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dummy_input = torch.zeros(2, 8).long()\n",
        "dummy_input[:,2:-2] = torch.arange(1,9).view(2,4)\n",
        "pp.pprint(dummy_input)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 1, 2, 3, 4, 0, 0],\n",
            "        [0, 0, 5, 6, 7, 8, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrXSoeBXnJcd",
        "outputId": "de19ded9-e09f-4af5-ef53-afc27b287272",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dummy_output = [[[dummy_input[i, j-2+k].item() for k in range(2*2+1)] \n",
        "                                                     for j in range(2, 6)] \n",
        "                                                            for i in range(2)]\n",
        "dummy_output = torch.LongTensor(dummy_output)\n",
        "print(dummy_output.size())\n",
        "pp.pprint(dummy_output)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 5])\n",
            "tensor([[[0, 0, 1, 2, 3],\n",
            "         [0, 1, 2, 3, 4],\n",
            "         [1, 2, 3, 4, 0],\n",
            "         [2, 3, 4, 0, 0]],\n",
            "\n",
            "        [[0, 0, 5, 6, 7],\n",
            "         [0, 5, 6, 7, 8],\n",
            "         [5, 6, 7, 8, 0],\n",
            "         [6, 7, 8, 0, 0]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9Nyb7OhnJcf"
      },
      "source": [
        "*Technically* it works: For each element in the batch, for each word in the original sentence and ignoring window padding, we've got the 5 token indices centered at that word. But in practice will be crazy slow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkw49b5EnJcg"
      },
      "source": [
        "Instead, we ideally want to find the right tensor operation in the PyTorch arsenal. Here, that happens to be __Tensor.unfold__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XV9ASMinJcg",
        "outputId": "2c425dfb-aa59-45a6-d530-1eb27ae9df4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dummy_input.unfold(1, 2*2+1, 1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0, 0, 1, 2, 3],\n",
              "         [0, 1, 2, 3, 4],\n",
              "         [1, 2, 3, 4, 0],\n",
              "         [2, 3, 4, 0, 0]],\n",
              "\n",
              "        [[0, 0, 5, 6, 7],\n",
              "         [0, 5, 6, 7, 8],\n",
              "         [5, 6, 7, 8, 0],\n",
              "         [6, 7, 8, 0, 0]]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCg7rPMInJcj"
      },
      "source": [
        "### A model in full."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqHovTS_nJcj"
      },
      "source": [
        "In PyTorch, we implement models by extending the nn.Module class. Minimally, this requires implementing an *\\_\\_init\\_\\_* function and a *forward* function.\n",
        "\n",
        "In *\\_\\_init\\_\\_* we want to store model parameters (weights) and hyperparameters (dimensions).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEX7mcuznJck"
      },
      "source": [
        "class SoftmaxWordWindowClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A one-layer, binary word-window classifier.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, vocab_size, pad_idx=0):\n",
        "        super(SoftmaxWordWindowClassifier, self).__init__()\n",
        "        \"\"\"\n",
        "        Instance variables.\n",
        "        \"\"\"\n",
        "        self.window_size = 2*config[\"half_window\"]+1\n",
        "        self.embed_dim = config[\"embed_dim\"]\n",
        "        self.hidden_dim = config[\"hidden_dim\"]\n",
        "        self.num_classes = config[\"num_classes\"]\n",
        "        self.freeze_embeddings = config[\"freeze_embeddings\"]\n",
        "        \n",
        "        \"\"\"\n",
        "        Embedding layer\n",
        "        -model holds an embedding for each layer in our vocab\n",
        "        -sets aside a special index in the embedding matrix for padding vector (of zeros)\n",
        "        -by default, embeddings are parameters (so gradients pass through them)\n",
        "        \"\"\"\n",
        "        self.embed_layer = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_idx)\n",
        "        if self.freeze_embeddings:\n",
        "            self.embed_layer.weight.requires_grad = False\n",
        "        \n",
        "        \"\"\"\n",
        "        Hidden layer\n",
        "        -we want to map embedded word windows of dim (window_size+1)*self.embed_dim to a hidden layer.\n",
        "        -nn.Sequential allows you to efficiently specify sequentially structured models\n",
        "            -first the linear transformation is evoked on the embedded word windows\n",
        "            -next the nonlinear transformation ReLU is evoked.\n",
        "        \"\"\"\n",
        "        self.hidden_layer = nn.Sequential(nn.Linear(self.window_size*self.embed_dim, \n",
        "                                                    self.hidden_dim), \n",
        "                                          nn.ReLU())\n",
        "        \n",
        "        \"\"\"\n",
        "        Output layer\n",
        "        -we want to map elements of the output layer (of size self.hidden dim) to a number of classes.\n",
        "        \"\"\"\n",
        "        self.output_layer = nn.Linear(self.hidden_dim, self.num_classes)\n",
        "        \n",
        "        \"\"\"\n",
        "        Softmax\n",
        "        -The final step of the softmax classifier: mapping final hidden layer to class scores.\n",
        "        -pytorch has both logsoftmax and softmax functions (and many others)\n",
        "        -since our loss is the negative LOG likelihood, we use logsoftmax\n",
        "        -technically you can take the softmax, and take the log but PyTorch's implementation\n",
        "         is optimized to avoid numerical underflow issues.\n",
        "        \"\"\"\n",
        "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Let B:= batch_size\n",
        "            L:= window-padded sentence length\n",
        "            D:= self.embed_dim\n",
        "            S:= self.window_size\n",
        "            H:= self.hidden_dim\n",
        "            \n",
        "        inputs: a (B, L) tensor of token indices\n",
        "        \"\"\"\n",
        "        B, L = inputs.size()\n",
        "        \n",
        "        \"\"\"\n",
        "        Reshaping.\n",
        "        Takes in a (B, L) LongTensor\n",
        "        Outputs a (B, L~, S) LongTensor\n",
        "        \"\"\"\n",
        "        # Fist, get our word windows for each word in our input.\n",
        "        token_windows = inputs.unfold(1, self.window_size, 1)\n",
        "        _, adjusted_length, _ = token_windows.size()\n",
        "        \n",
        "        # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
        "        assert token_windows.size() == (B, adjusted_length, self.window_size)\n",
        "        \n",
        "        \"\"\"\n",
        "        Embedding.\n",
        "        Takes in a torch.LongTensor of size (B, L~, S) \n",
        "        Outputs a (B, L~, S, D) FloatTensor.\n",
        "        \"\"\"\n",
        "        embedded_windows = self.embed_layer(token_windows)\n",
        "        \n",
        "        \"\"\"\n",
        "        Reshaping.\n",
        "        Takes in a (B, L~, S, D) FloatTensor.\n",
        "        Resizes it into a (B, L~, S*D) FloatTensor.\n",
        "        -1 argument \"infers\" what the last dimension should be based on leftover axes.\n",
        "        \"\"\"\n",
        "        embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
        "        \n",
        "        \"\"\"\n",
        "        Layer 1.\n",
        "        Takes in a (B, L~, S*D) FloatTensor.\n",
        "        Resizes it into a (B, L~, H) FloatTensor\n",
        "        \"\"\"\n",
        "        layer_1 = self.hidden_layer(embedded_windows)\n",
        "        \n",
        "        \"\"\"\n",
        "        Layer 2\n",
        "        Takes in a (B, L~, H) FloatTensor.\n",
        "        Resizes it into a (B, L~, 2) FloatTensor.\n",
        "        \"\"\"\n",
        "        output = self.output_layer(layer_1)\n",
        "        \n",
        "        \"\"\"\n",
        "        Softmax.\n",
        "        Takes in a (B, L~, 2) FloatTensor of unnormalized class scores.\n",
        "        Outputs a (B, L~, 2) FloatTensor of (log-)normalized class scores.\n",
        "        \"\"\"\n",
        "        output = self.log_softmax(output)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUqPxbb6nJcn"
      },
      "source": [
        "### Training.\n",
        "\n",
        "Now that we've got a model, we have to train it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6n-AXs_nJcn"
      },
      "source": [
        "def loss_function(outputs, labels, lengths):\n",
        "    \"\"\"Computes negative LL loss on a batch of model predictions.\"\"\"\n",
        "    B, L, num_classes = outputs.size()\n",
        "    num_elems = lengths.sum().float()\n",
        "        \n",
        "    # get only the values with non-zero labels\n",
        "    loss = outputs*labels\n",
        "    \n",
        "    # rescale average\n",
        "    return -loss.sum() / num_elems"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB_oOfTsnJcp"
      },
      "source": [
        "def train_epoch(loss_function, optimizer, model, train_data):\n",
        "    \n",
        "    ## For each batch, we must reset the gradients\n",
        "    ## stored by the model.   \n",
        "    total_loss = 0\n",
        "    for batch, labels, lengths in train_data:\n",
        "        # clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        # evoke model in training mode on batch\n",
        "        outputs = model.forward(batch)\n",
        "        # compute loss w.r.t batch\n",
        "        loss = loss_function(outputs, labels, lengths)\n",
        "        # pass gradients back, startiing on loss value\n",
        "        loss.backward()\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    # return the total to keep track of how you did this time around\n",
        "    return total_loss\n",
        "    "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-ZyOq-UnJcs"
      },
      "source": [
        "config = {\"batch_size\": 4,\n",
        "          \"half_window\": 2,\n",
        "          \"embed_dim\": 25,\n",
        "          \"hidden_dim\": 25,\n",
        "          \"num_classes\": 2,\n",
        "          \"freeze_embeddings\": False,\n",
        "         }\n",
        "learning_rate = .0002\n",
        "num_epochs = 10000\n",
        "model = SoftmaxWordWindowClassifier(config, len(word_2_id))\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOqrs9bPMRho"
      },
      "source": [
        "Let's take a peek at our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMezXV88MPma",
        "outputId": "02d61673-86d5-4357-d0fb-da6d5c6378ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SoftmaxWordWindowClassifier(\n",
              "  (embed_layer): Embedding(18, 25, padding_idx=0)\n",
              "  (hidden_layer): Sequential(\n",
              "    (0): Linear(in_features=125, out_features=25, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (output_layer): Linear(in_features=25, out_features=2, bias=True)\n",
              "  (log_softmax): LogSoftmax(dim=2)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xjjz3NWnJcu"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(list(zip(train_sents, train_labels)), \n",
        "                                           batch_size=2, \n",
        "                                           shuffle=True, \n",
        "                                           collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "DPGySROsnJcw",
        "outputId": "1f388c4e-9110-4dcd-8508-433fd12d8df8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "95f6b111c74d4602aaeb4059f9f25774",
            "e035ffed832c4b3f9f439b7a126ced0c",
            "578610b1b5cf46ef828278aa1f3a1273",
            "9bae9b2cba054689b254b45a89cd45e3",
            "261801c8e972468fa4b1e1e1660cd7db",
            "16b730af931f476db36ca3498c9bd6d7",
            "c48114ad9c3e45538e36730a828e31cf",
            "c7822ebcd37b4aa8a29f5fad505d1806",
            "9edd66f814d94a3cb5e382f7a4cf02a8",
            "620d3464579249e698043cc5d57f098a",
            "027beeea2a4a49d58d5d26eedf6f9959"
          ]
        }
      },
      "source": [
        "for epoch in tqdm(range(num_epochs)):\n",
        "    epoch_loss = train_epoch(loss_function, optimizer, model, train_loader)\n",
        "    if epoch % 100 == 0:\n",
        "        print(epoch_loss)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95f6b111c74d4602aaeb4059f9f25774",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.4764497876167297\n",
            "1.387013554573059\n",
            "1.309329330921173\n",
            "1.2552019357681274\n",
            "1.193853497505188\n",
            "1.1311131715774536\n",
            "1.0919742584228516\n",
            "1.045390009880066\n",
            "1.0033448934555054\n",
            "0.9641788303852081\n",
            "0.9288147687911987\n",
            "0.8934848606586456\n",
            "0.858789712190628\n",
            "0.8283379375934601\n",
            "0.801212340593338\n",
            "0.7713947892189026\n",
            "0.7449979484081268\n",
            "0.7199913561344147\n",
            "0.6957535743713379\n",
            "0.67049440741539\n",
            "0.6499781310558319\n",
            "0.6287527084350586\n",
            "0.6066800355911255\n",
            "0.589207112789154\n",
            "0.5687434673309326\n",
            "0.5556463301181793\n",
            "0.5351220369338989\n",
            "0.5217765718698502\n",
            "0.5056260973215103\n",
            "0.48998206853866577\n",
            "0.4685928523540497\n",
            "0.45603740215301514\n",
            "0.44630493223667145\n",
            "0.4260224997997284\n",
            "0.4192611873149872\n",
            "0.40173348784446716\n",
            "0.3941297382116318\n",
            "0.37550053000450134\n",
            "0.36625850200653076\n",
            "0.35301798582077026\n",
            "0.34458693861961365\n",
            "0.334529384970665\n",
            "0.3248341828584671\n",
            "0.315464049577713\n",
            "0.30623288452625275\n",
            "0.30225910246372223\n",
            "0.28658542037010193\n",
            "0.2855807915329933\n",
            "0.2704756110906601\n",
            "0.2628595009446144\n",
            "0.25551289319992065\n",
            "0.2507256492972374\n",
            "0.24387961626052856\n",
            "0.24209639430046082\n",
            "0.2286217138171196\n",
            "0.224727101624012\n",
            "0.21654970943927765\n",
            "0.2130354791879654\n",
            "0.2121027559041977\n",
            "0.1999696120619774\n",
            "0.19482450932264328\n",
            "0.1964261159300804\n",
            "0.1871250569820404\n",
            "0.1824566051363945\n",
            "0.17793995887041092\n",
            "0.1716022863984108\n",
            "0.16937843710184097\n",
            "0.16945676505565643\n",
            "0.16140298545360565\n",
            "0.16162284463644028\n",
            "0.1578907147049904\n",
            "0.15039243549108505\n",
            "0.14521441608667374\n",
            "0.1474165916442871\n",
            "0.14413880184292793\n",
            "0.13565672934055328\n",
            "0.13429244980216026\n",
            "0.12978358939290047\n",
            "0.13203467056155205\n",
            "0.12923775240778923\n",
            "0.12164155766367912\n",
            "0.12389246746897697\n",
            "0.12133113667368889\n",
            "0.11422038078308105\n",
            "0.11644883453845978\n",
            "0.11411081627011299\n",
            "0.10877655446529388\n",
            "0.10662388056516647\n",
            "0.10751108080148697\n",
            "0.10250458493828773\n",
            "0.09930860251188278\n",
            "0.09741828218102455\n",
            "0.099542286247015\n",
            "0.0949535109102726\n",
            "0.09589506685733795\n",
            "0.09413569793105125\n",
            "0.08982435241341591\n",
            "0.08820512518286705\n",
            "0.08663361892104149\n",
            "0.08408811688423157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1hnCH3inJcy"
      },
      "source": [
        "### Prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-0QocWknJcz"
      },
      "source": [
        "test_loader = torch.utils.data.DataLoader(list(zip(test_sents, test_labels)), \n",
        "                                           batch_size=1, \n",
        "                                           shuffle=False, \n",
        "                                           collate_fn=partial(my_collate, window_size=2, word_2_id=word_2_id))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNgomU9OnJc1",
        "outputId": "406e30c0-fd4d-40d1-c650-68f4ecea1e2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for test_instance, labs, _ in test_loader:\n",
        "    outputs = model.forward(test_instance)\n",
        "    print(torch.argmax(outputs, dim=2))\n",
        "    print(torch.argmax(labs, dim=2))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0, 1]])\n",
            "tensor([[0, 0, 0, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wanna take it to the next level?"
      ],
      "metadata": {
        "id": "K79ukzQC8QMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are annotated sentences from CONLL-2013.\n",
        "They are divided into train, dev and test sets.\n",
        "\n",
        "Each line represents a word, together with its annotation: B-ORG is the first (Begin) word of an organization. \n",
        "\n",
        "B-PER and I-PER represents the *B*egin and *I*nner word names of a person."
      ],
      "metadata": {
        "id": "rpudI19p8UZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "data = {}\n",
        "for t in ['train', 'test', 'dev']:\n",
        "  response = requests.get(\"https://github.com/ZihanWangKi/CrossWeigh/raw/master/data/conllpp_train.txt\")\n",
        "  data[t] = response.text"
      ],
      "metadata": {
        "id": "cMBv9Idf54JX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['train'][:220])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN4rAfTE8pJx",
        "outputId": "60bff127-a002-4f7d-813d-552f8501170a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-DOCSTART- -X- -X- O\n",
            "\n",
            "EU NNP B-NP B-ORG\n",
            "rejects VBZ B-VP O\n",
            "German JJ B-NP B-MISC\n",
            "call NN I-NP O\n",
            "to TO B-VP O\n",
            "boycott VB I-VP O\n",
            "British JJ B-NP B-MISC\n",
            "lamb NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Peter NNP B-NP B-PER\n",
            "Blackburn NNP I-NP I-PER\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try parsing the file in a way that you can use with the softmax model, and re-run the training. How well are your results?"
      ],
      "metadata": {
        "id": "fNkmorLQFDwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-1eLoZQk8sJ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}