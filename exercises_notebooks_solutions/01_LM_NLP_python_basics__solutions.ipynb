{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_LM_NLP_python_basics__solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1OqX9GeN216c"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liadmagen/NLP-Course/blob/master/exercises_notebooks_solutions/01_LM_NLP_python_basics__solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF7evQxY31Kr"
      },
      "source": [
        "We are going to use the package `NLTK` - 'Natural Language Toolkit' (https://www.nltk.org/).\n",
        "\n",
        "NLTK is a great package for research and for learning. However, it isn't recommended for production use and for real-world applications, as it isn't fast enough and therefore doesn't scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OqX9GeN216c"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS2o9QMaUmzC"
      },
      "source": [
        "import random\n",
        "\n",
        "import nltk"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gBd4ZgXzAmP",
        "outputId": "622c6380-ce17-4e21-a2af-c2dfb9bdf468",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('book')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRzGNH63xcwQ",
        "outputId": "9486f458-7b22-4cb7-d04a-e9896d156260",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.book import *"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlR3eNPu3EWS"
      },
      "source": [
        "# A Closer Look at Python: Texts as Lists of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul9GAlK63sMj"
      },
      "source": [
        "We will use the great book 'Moby Dick' by Herman Melville, as our learning experiment playground.\n",
        "\n",
        "The book is already tokenized and stored as a list of these tokens, under the variable `text1`.\n",
        "\n",
        "We start - as always - with looking at our data. \n",
        "\n",
        "Let's peek at the first 100 words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjIvrcow3Pjc",
        "outputId": "c713a108-424a-4742-e5d2-19df6b804c9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text1[:100]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'Moby',\n",
              " 'Dick',\n",
              " 'by',\n",
              " 'Herman',\n",
              " 'Melville',\n",
              " '1851',\n",
              " ']',\n",
              " 'ETYMOLOGY',\n",
              " '.',\n",
              " '(',\n",
              " 'Supplied',\n",
              " 'by',\n",
              " 'a',\n",
              " 'Late',\n",
              " 'Consumptive',\n",
              " 'Usher',\n",
              " 'to',\n",
              " 'a',\n",
              " 'Grammar',\n",
              " 'School',\n",
              " ')',\n",
              " 'The',\n",
              " 'pale',\n",
              " 'Usher',\n",
              " '--',\n",
              " 'threadbare',\n",
              " 'in',\n",
              " 'coat',\n",
              " ',',\n",
              " 'heart',\n",
              " ',',\n",
              " 'body',\n",
              " ',',\n",
              " 'and',\n",
              " 'brain',\n",
              " ';',\n",
              " 'I',\n",
              " 'see',\n",
              " 'him',\n",
              " 'now',\n",
              " '.',\n",
              " 'He',\n",
              " 'was',\n",
              " 'ever',\n",
              " 'dusting',\n",
              " 'his',\n",
              " 'old',\n",
              " 'lexicons',\n",
              " 'and',\n",
              " 'grammars',\n",
              " ',',\n",
              " 'with',\n",
              " 'a',\n",
              " 'queer',\n",
              " 'handkerchief',\n",
              " ',',\n",
              " 'mockingly',\n",
              " 'embellished',\n",
              " 'with',\n",
              " 'all',\n",
              " 'the',\n",
              " 'gay',\n",
              " 'flags',\n",
              " 'of',\n",
              " 'all',\n",
              " 'the',\n",
              " 'known',\n",
              " 'nations',\n",
              " 'of',\n",
              " 'the',\n",
              " 'world',\n",
              " '.',\n",
              " 'He',\n",
              " 'loved',\n",
              " 'to',\n",
              " 'dust',\n",
              " 'his',\n",
              " 'old',\n",
              " 'grammars',\n",
              " ';',\n",
              " 'it',\n",
              " 'somehow',\n",
              " 'mildly',\n",
              " 'reminded',\n",
              " 'him',\n",
              " 'of',\n",
              " 'his',\n",
              " 'mortality',\n",
              " '.',\n",
              " '\"',\n",
              " 'While',\n",
              " 'you',\n",
              " 'take',\n",
              " 'in',\n",
              " 'hand',\n",
              " 'to',\n",
              " 'school',\n",
              " 'others',\n",
              " ',']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD5ADOnC48oM"
      },
      "source": [
        "**Pay attention that punctuations are also conisdered as a `token`.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uthWKmm153r-"
      },
      "source": [
        "Exercise #1: Show the last 23 tokens in the book:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvWRtYl-3Ubi",
        "outputId": "225b93fd-537a-4bd9-c85d-42fb502dc768",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### your turn: Write a code that shows the last sentence (23 tokens) of the book\n",
        "text1[-23:]\n",
        "### End"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It',\n",
              " 'was',\n",
              " 'the',\n",
              " 'devious',\n",
              " '-',\n",
              " 'cruising',\n",
              " 'Rachel',\n",
              " ',',\n",
              " 'that',\n",
              " 'in',\n",
              " 'her',\n",
              " 'retracing',\n",
              " 'search',\n",
              " 'after',\n",
              " 'her',\n",
              " 'missing',\n",
              " 'children',\n",
              " ',',\n",
              " 'only',\n",
              " 'found',\n",
              " 'another',\n",
              " 'orphan',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exRELqhv_g20"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC0w4C5T68wF"
      },
      "source": [
        "In python, an ordered set, with repetition, is defined as a List, and marked with sqaured brackets [].\n",
        "\n",
        "An unordered set, where repetitions are discarded, is defined with regular brackets: ().\n",
        "\n",
        "When converting the list into a set, we get the vocabulary of the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsUBYcwl6Cdc",
        "outputId": "86e5c15b-fc40-4d2e-b99f-738e69a63f53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab = set(text1)\n",
        "\n",
        "# We can't get the 'last 25 words' of the vocabulary, since the set has no order \n",
        "# But we can convert it into a list first, and even sort it:\n",
        "list(sorted(vocab))[-25:]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['yon',\n",
              " 'yonder',\n",
              " 'yore',\n",
              " 'you',\n",
              " 'young',\n",
              " 'younger',\n",
              " 'youngest',\n",
              " 'youngish',\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourselbs',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'youth',\n",
              " 'youthful',\n",
              " 'zag',\n",
              " 'zay',\n",
              " 'zeal',\n",
              " 'zephyr',\n",
              " 'zig',\n",
              " 'zodiac',\n",
              " 'zone',\n",
              " 'zoned',\n",
              " 'zones',\n",
              " 'zoology']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkvijxKv8nqm"
      },
      "source": [
        "Exercise #2: How many words does our vocabulary have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJnFJcsG8k2X",
        "outputId": "960c28bb-fc49-430e-9dcb-44946b36b2b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### your turn: Write a code that prints the size of Moby Dick book's vocabulary\n",
        "len(vocab)\n",
        "### End"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19317"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxlwVLXG9BRg"
      },
      "source": [
        "# Text Analysis: Frequency Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlpuCzpK9H9J"
      },
      "source": [
        "nltk is a library with many research tools for probabilistic information. \n",
        "\n",
        "For example, it includes a function, `FreqDist`, that return the probability of the occurance of a word in a text:\n",
        "\n",
        "http://www.nltk.org/api/nltk.html?highlight=freqdist#module-nltk.probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z76z5LY19FQi",
        "outputId": "19d6335e-1d1a-4e93-d3d0-48de86f02860",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## write a code that calculate the frequency of words in text1 and prints the top 50 common ones.\n",
        "## How many times do the words 'with', 'Moby', 'fish' and 'whale' appear in the book?\n",
        "## hint - fdist is a smart dictionary that already has methods for these tasks, \n",
        "## such as .most_common() \n",
        "\n",
        "freqdist = FreqDist(text1)\n",
        "print(freqdist.most_common(20))\n",
        "\n",
        "for w in ['with', 'Moby', 'fish', 'whale']:\n",
        "  print(f\"{w}: {freqdist[w]}\")\n",
        "\n",
        "### End"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024), ('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982), (\"'\", 2684), ('-', 2552), ('his', 2459), ('it', 2209), ('I', 2124), ('s', 1739), ('is', 1695), ('he', 1661), ('with', 1659), ('was', 1632)]\n",
            "with: 1659\n",
            "Moby: 84\n",
            "fish: 133\n",
            "whale: 906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F39jB_1jAboe"
      },
      "source": [
        "Some of the common words are actually punctuations and 'stop-words'. They don't really help us with our analysis of the text, and therefore should be ignored.\n",
        "\n",
        "Luckily, NLTK supplies a list of stop words, and python has the punctuation built in into the string package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qne74woEATJo",
        "outputId": "eb53feef-0a65-4fca-e822-09cba9bf4983",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "print(stopwords.words('english'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NWLKWf79z94",
        "outputId": "b1c133e6-4f37-4082-edef-651a8c6ffdfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import string\n",
        "\n",
        "print(string.punctuation)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APuonB9pA1T3",
        "outputId": "c9f9535e-b310-4104-8e08-0cb95ef795de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "### Find and print the top 50 frequennt words, without stop words or punctuation.\n",
        "### Hint: like the mathematical sets, a python Set has an ability to intersect, detect subsets and even subtract:\n",
        "### See more in here: https://docs.python.org/3.8/library/stdtypes.html#set\n",
        "\n",
        "punct = set(string.punctuation)\n",
        "sw = set(stopwords.words('english'))\n",
        "both = punct.union(sw)\n",
        "\n",
        "freqdist = FreqDist(w for w in text1 if w.lower() not in both)\n",
        "print(freqdist.most_common(50))\n",
        "\n",
        "\n",
        "###"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('--', 1070), ('whale', 906), ('one', 889), ('like', 624), ('upon', 538), ('man', 508), ('ship', 507), ('Ahab', 501), ('.\"', 489), ('ye', 460), ('old', 436), ('sea', 433), ('would', 421), ('head', 335), ('though', 335), ('boat', 330), ('time', 324), ('long', 318), ('!\"', 305), ('said', 302), (',\"', 302), ('yet', 300), ('still', 299), ('great', 293), ('two', 285), ('seemed', 283), ('must', 282), ('Whale', 282), ('last', 277), ('way', 269), ('Stubb', 255), ('see', 253), ('?\"', 252), ('Queequeg', 252), ('little', 247), ('round', 242), ('whales', 237), ('say', 237), ('three', 237), ('men', 236), ('thou', 232), ('may', 230), ('us', 228), ('every', 222), ('much', 218), ('could', 215), ('Captain', 215), ('first', 210), ('side', 208), ('hand', 205)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGCb2GiEEuqM"
      },
      "source": [
        "FreqDist can be used even further. Let's analyse the text by the word length.\n",
        "\n",
        "Using python 'list-comprehension' (https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions) we can easily get a list of all the words by their lengths:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBYSet5WEtcS",
        "outputId": "5dacb367-c45e-4422-fc8e-bf7396c1da45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# For convenience of reading, showing here only the first 30\n",
        "[len(w) for w in text1][:30]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 4,\n",
              " 4,\n",
              " 2,\n",
              " 6,\n",
              " 8,\n",
              " 4,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 1,\n",
              " 8,\n",
              " 2,\n",
              " 1,\n",
              " 4,\n",
              " 11,\n",
              " 5,\n",
              " 2,\n",
              " 1,\n",
              " 7,\n",
              " 6,\n",
              " 1,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 2,\n",
              " 10,\n",
              " 2,\n",
              " 4,\n",
              " 1]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgHoCYA-FXDa"
      },
      "source": [
        "### Write a code to calculate the frequency of the length of words in `text`. \n",
        "### How often do the 20 most lengthiest words appear in the text?\n",
        "### Extra: Find out what those 20 words are (hint: a python dictionary has a .keys() method)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### End"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Buil9-Z-Emvk"
      },
      "source": [
        "# Text Analusis: n-grams and collocation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVKEjI4GCXei"
      },
      "source": [
        "As we learnt in class, a word is not always a single token. In the case of 'New York', 'ice cream', 'red wine', etc., a single word meaning is different than the combined one.\n",
        "\n",
        "A **collocation** is a sequence of words that occur together unusually often.\n",
        "\n",
        "An `n-gram` is a sequence of a size of 'n' of tokens (i.e. words):\n",
        "\n",
        "* When n=1: it is called **unigram**\n",
        "* When n=2: it is called **bigram**\n",
        "* When n=3: it is called **trigram** ...\n",
        "* When n>3: it is just called an **n-gram** with the size of 4.\n",
        "\n",
        "\n",
        "NLTK has two functions: `bigrams` and `collocations`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itg3dYQ3DeSG",
        "outputId": "c7697561-1c69-446c-f220-f00642d8d936",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(bigrams([1,2,3,4,5]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 2), (2, 3), (3, 4), (4, 5)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HosjcKYuEIac",
        "outputId": "d8474a01-35d9-4dab-e70f-64b19a228399",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "## Bigrams generates bi-grams from the text: every two words would be collected together.\n",
        "list(bigrams(text1))[:20]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('[', 'Moby'),\n",
              " ('Moby', 'Dick'),\n",
              " ('Dick', 'by'),\n",
              " ('by', 'Herman'),\n",
              " ('Herman', 'Melville'),\n",
              " ('Melville', '1851'),\n",
              " ('1851', ']'),\n",
              " (']', 'ETYMOLOGY'),\n",
              " ('ETYMOLOGY', '.'),\n",
              " ('.', '('),\n",
              " ('(', 'Supplied'),\n",
              " ('Supplied', 'by'),\n",
              " ('by', 'a'),\n",
              " ('a', 'Late'),\n",
              " ('Late', 'Consumptive'),\n",
              " ('Consumptive', 'Usher'),\n",
              " ('Usher', 'to'),\n",
              " ('to', 'a'),\n",
              " ('a', 'Grammar'),\n",
              " ('Grammar', 'School')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUqPHOytDfBx",
        "outputId": "f6b7ee77-ceb5-4d46-e660-0ee22a6f871b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text1.collocations()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n",
            "whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n",
            "years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n",
            "mate; white whale; ivory leg; one hand\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zko0TVbLHWbl"
      },
      "source": [
        "# Python and NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2vqtYKYHaPa"
      },
      "source": [
        "Python has many strong capabilities, built in, when it comes to string and text procesing, in combined with the list comprehension.\n",
        "\n",
        "Here are some examples of filtering the word list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWs-o3xoHZck",
        "outputId": "d0793c8b-8314-4d5f-9baa-04496269989c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get all the words that ends with 'ableness', sorted:\n",
        "sorted(w for w in set(text1) if w.endswith('ableness'))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['comfortableness',\n",
              " 'honourableness',\n",
              " 'immutableness',\n",
              " 'indispensableness',\n",
              " 'indomitableness',\n",
              " 'intolerableness',\n",
              " 'palpableness',\n",
              " 'reasonableness',\n",
              " 'uncomfortableness']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1Dp1G3nHv3N",
        "outputId": "7d002985-6528-48c8-8f61-fd5432b65e8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get all the words that contains 'orate', sorted:\n",
        "sorted(term for term in set(text1) if 'orate' in term)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['camphorated',\n",
              " 'corroborated',\n",
              " 'decorated',\n",
              " 'elaborate',\n",
              " 'elaborately',\n",
              " 'evaporate',\n",
              " 'evaporates',\n",
              " 'incorporate',\n",
              " 'incorporated']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxVjcOg_H1e8",
        "outputId": "40761868-480b-40c8-bbdd-1294137e1d56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get all the words which their first letter is capitalized:\n",
        "sorted(item for item in set(text1) if item.istitle())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3D',\n",
              " 'A',\n",
              " 'Abashed',\n",
              " 'Abednego',\n",
              " 'Abel',\n",
              " 'Abjectus',\n",
              " 'Aboard',\n",
              " 'Abominable',\n",
              " 'About',\n",
              " 'Above',\n",
              " 'Abraham',\n",
              " 'Academy',\n",
              " 'Accessory',\n",
              " 'According',\n",
              " 'Accordingly',\n",
              " 'Accursed',\n",
              " 'Achilles',\n",
              " 'Actium',\n",
              " 'Acushnet',\n",
              " 'Adam',\n",
              " 'Adieu',\n",
              " 'Adios',\n",
              " 'Admiral',\n",
              " 'Admirals',\n",
              " 'Advance',\n",
              " 'Advancement',\n",
              " 'Adventures',\n",
              " 'Adverse',\n",
              " 'Advocate',\n",
              " 'Affected',\n",
              " 'Affidavit',\n",
              " 'Affrighted',\n",
              " 'Afric',\n",
              " 'Africa',\n",
              " 'African',\n",
              " 'Africans',\n",
              " 'Aft',\n",
              " 'After',\n",
              " 'Afterwards',\n",
              " 'Again',\n",
              " 'Against',\n",
              " 'Agassiz',\n",
              " 'Ages',\n",
              " 'Ah',\n",
              " 'Ahab',\n",
              " 'Ahabs',\n",
              " 'Ahasuerus',\n",
              " 'Ahaz',\n",
              " 'Ahoy',\n",
              " 'Ain',\n",
              " 'Air',\n",
              " 'Akin',\n",
              " 'Alabama',\n",
              " 'Aladdin',\n",
              " 'Alarmed',\n",
              " 'Alas',\n",
              " 'Albatross',\n",
              " 'Albemarle',\n",
              " 'Albert',\n",
              " 'Albicore',\n",
              " 'Albino',\n",
              " 'Aldrovandi',\n",
              " 'Aldrovandus',\n",
              " 'Alexander',\n",
              " 'Alexanders',\n",
              " 'Alfred',\n",
              " 'Algerine',\n",
              " 'Algiers',\n",
              " 'Alike',\n",
              " 'Alive',\n",
              " 'All',\n",
              " 'Alleghanian',\n",
              " 'Alleghanies',\n",
              " 'Alley',\n",
              " 'Almanack',\n",
              " 'Almighty',\n",
              " 'Almost',\n",
              " 'Aloft',\n",
              " 'Alone',\n",
              " 'Alps',\n",
              " 'Already',\n",
              " 'Also',\n",
              " 'Am',\n",
              " 'Ambergriese',\n",
              " 'Ambergris',\n",
              " 'Amelia',\n",
              " 'America',\n",
              " 'American',\n",
              " 'Americans',\n",
              " 'Americas',\n",
              " 'Amittai',\n",
              " 'Among',\n",
              " 'Amsterdam',\n",
              " 'An',\n",
              " 'Anacharsis',\n",
              " 'Anak',\n",
              " 'Anatomist',\n",
              " 'And',\n",
              " 'Andes',\n",
              " 'Andrew',\n",
              " 'Andromeda',\n",
              " 'Angel',\n",
              " 'Angelo',\n",
              " 'Angels',\n",
              " 'Animated',\n",
              " 'Annawon',\n",
              " 'Anne',\n",
              " 'Anno',\n",
              " 'Anomalous',\n",
              " 'Another',\n",
              " 'Answer',\n",
              " 'Antarctic',\n",
              " 'Antilles',\n",
              " 'Antiochus',\n",
              " 'Antony',\n",
              " 'Antwerp',\n",
              " 'Anvil',\n",
              " 'Any',\n",
              " 'Anyhow',\n",
              " 'Anything',\n",
              " 'Anyway',\n",
              " 'Apollo',\n",
              " 'Apoplexy',\n",
              " 'Applied',\n",
              " 'Apply',\n",
              " 'April',\n",
              " 'Aquarius',\n",
              " 'Arch',\n",
              " 'Archbishop',\n",
              " 'Arched',\n",
              " 'Archer',\n",
              " 'Archipelagoes',\n",
              " 'Archy',\n",
              " 'Arctic',\n",
              " 'Are',\n",
              " 'Arethusa',\n",
              " 'Argo',\n",
              " 'Aries',\n",
              " 'Arion',\n",
              " 'Aristotle',\n",
              " 'Ark',\n",
              " 'Arkansas',\n",
              " 'Arkite',\n",
              " 'Arm',\n",
              " 'Armada',\n",
              " 'Arnold',\n",
              " 'Aroostook',\n",
              " 'Around',\n",
              " 'Arrayed',\n",
              " 'Arrived',\n",
              " 'Arsacidean',\n",
              " 'Arsacides',\n",
              " 'Art',\n",
              " 'Artedi',\n",
              " 'Arter',\n",
              " 'Articles',\n",
              " 'As',\n",
              " 'Asa',\n",
              " 'Ashantee',\n",
              " 'Ashore',\n",
              " 'Asia',\n",
              " 'Asiatic',\n",
              " 'Asiatics',\n",
              " 'Aside',\n",
              " 'Asphaltites',\n",
              " 'Assaulted',\n",
              " 'Assume',\n",
              " 'Assuming',\n",
              " 'Assuredly',\n",
              " 'Assyrian',\n",
              " 'Astern',\n",
              " 'Astir',\n",
              " 'Astronomy',\n",
              " 'At',\n",
              " 'Atlantic',\n",
              " 'Atlantics',\n",
              " 'Attached',\n",
              " 'Attend',\n",
              " 'August',\n",
              " 'Aunt',\n",
              " 'Australia',\n",
              " 'Australian',\n",
              " 'Austrian',\n",
              " 'Author',\n",
              " 'Authors',\n",
              " 'Auto',\n",
              " 'Availing',\n",
              " 'Avast',\n",
              " 'Avatar',\n",
              " 'Aware',\n",
              " 'Away',\n",
              " 'Awful',\n",
              " 'Ay',\n",
              " 'Aye',\n",
              " 'Azores',\n",
              " 'Babel',\n",
              " 'Babylon',\n",
              " 'Babylonian',\n",
              " 'Bachelor',\n",
              " 'Back',\n",
              " 'Backs',\n",
              " 'Bad',\n",
              " 'Baden',\n",
              " 'Bag',\n",
              " 'Balaene',\n",
              " 'Baliene',\n",
              " 'Baling',\n",
              " 'Bally',\n",
              " 'Baltic',\n",
              " 'Baltimore',\n",
              " 'Bamboo',\n",
              " 'Bang',\n",
              " 'Banks',\n",
              " 'Barbary',\n",
              " 'Bare',\n",
              " 'Bargain',\n",
              " 'Baron',\n",
              " 'Barrens',\n",
              " 'Bartholomew',\n",
              " 'Base',\n",
              " 'Bashaw',\n",
              " 'Bashee',\n",
              " 'Basilosaurus',\n",
              " 'Bastille',\n",
              " 'Battering',\n",
              " 'Battery',\n",
              " 'Bay',\n",
              " 'Bays',\n",
              " 'Be',\n",
              " 'Beach',\n",
              " 'Beale',\n",
              " 'Beams',\n",
              " 'Bear',\n",
              " 'Bears',\n",
              " 'Beat',\n",
              " 'Because',\n",
              " 'Becket',\n",
              " 'Bedford',\n",
              " 'Beelzebub',\n",
              " 'Befooled',\n",
              " 'Before',\n",
              " 'Begone',\n",
              " 'Behold',\n",
              " 'Behring',\n",
              " 'Being',\n",
              " 'Belated',\n",
              " 'Belial',\n",
              " 'Believe',\n",
              " 'Belisarius',\n",
              " 'Bell',\n",
              " 'Bellies',\n",
              " 'Beloved',\n",
              " 'Below',\n",
              " 'Belshazzar',\n",
              " 'Belubed',\n",
              " 'Bench',\n",
              " 'Bendigoes',\n",
              " 'Beneath',\n",
              " 'Bengal',\n",
              " 'Benjamin',\n",
              " 'Bennett',\n",
              " 'Bentham',\n",
              " 'Berkshire',\n",
              " 'Berlin',\n",
              " 'Bernard',\n",
              " 'Besides',\n",
              " 'Bess',\n",
              " 'Best',\n",
              " 'Bestow',\n",
              " 'Bethink',\n",
              " 'Better',\n",
              " 'Betty',\n",
              " 'Between',\n",
              " 'Beware',\n",
              " 'Beyond',\n",
              " 'Bible',\n",
              " 'Bibles',\n",
              " 'Bibliographical',\n",
              " 'Bildad',\n",
              " 'Biographical',\n",
              " 'Birmah',\n",
              " 'Bishop',\n",
              " 'Bite',\n",
              " 'Black',\n",
              " 'Blacksmith',\n",
              " 'Blackstone',\n",
              " 'Blanc',\n",
              " 'Blanche',\n",
              " 'Blanco',\n",
              " 'Blang',\n",
              " 'Blanket',\n",
              " 'Blast',\n",
              " 'Bless',\n",
              " 'Blind',\n",
              " 'Blinding',\n",
              " 'Blocksburg',\n",
              " 'Blood',\n",
              " 'Bloody',\n",
              " 'Blue',\n",
              " 'Boat',\n",
              " 'Boats',\n",
              " 'Bobbing',\n",
              " 'Bolivia',\n",
              " 'Bombay',\n",
              " 'Bonapartes',\n",
              " 'Bone',\n",
              " 'Bones',\n",
              " 'Bonneterre',\n",
              " 'Booble',\n",
              " 'Book',\n",
              " 'Boomer',\n",
              " 'Boone',\n",
              " 'Bordeaux',\n",
              " 'Borean',\n",
              " 'Born',\n",
              " 'Borneo',\n",
              " 'Bosom',\n",
              " 'Boston',\n",
              " 'Both',\n",
              " 'Bottle',\n",
              " 'Bottom',\n",
              " 'Bourbons',\n",
              " 'Bout',\n",
              " 'Bouton',\n",
              " 'Bowditch',\n",
              " 'Bower',\n",
              " 'Boy',\n",
              " 'Boys',\n",
              " 'Brace',\n",
              " 'Brahma',\n",
              " 'Brahmins',\n",
              " 'Brandreth',\n",
              " 'Brazil',\n",
              " 'Breakfast',\n",
              " 'Bremen',\n",
              " 'Bress',\n",
              " 'Bridge',\n",
              " 'Brighggians',\n",
              " 'Bright',\n",
              " 'Bring',\n",
              " 'Brisson',\n",
              " 'Brit',\n",
              " 'Britain',\n",
              " 'British',\n",
              " 'Britons',\n",
              " 'Broad',\n",
              " 'Broadway',\n",
              " 'Broke',\n",
              " 'Brother',\n",
              " 'Browne',\n",
              " 'Brute',\n",
              " 'Buckets',\n",
              " 'Bud',\n",
              " 'Buffalo',\n",
              " 'Bulkington',\n",
              " 'Bull',\n",
              " 'Bulwarks',\n",
              " 'Bunger',\n",
              " 'Bungle',\n",
              " 'Bunyan',\n",
              " 'Buoy',\n",
              " 'Buoyed',\n",
              " 'Burke',\n",
              " 'Burkes',\n",
              " 'Burst',\n",
              " 'Burton',\n",
              " 'Burtons',\n",
              " 'Business',\n",
              " 'But',\n",
              " 'Butchers',\n",
              " 'Butler',\n",
              " 'By',\n",
              " 'Byward',\n",
              " 'C',\n",
              " 'Cabaco',\n",
              " 'Cabin',\n",
              " 'Cachalot',\n",
              " 'Cadiz',\n",
              " 'Caesar',\n",
              " 'Caesarian',\n",
              " 'Cain',\n",
              " 'Calais',\n",
              " 'Californian',\n",
              " 'Call',\n",
              " 'Callao',\n",
              " 'Cambyses',\n",
              " 'Camel',\n",
              " 'Campagna',\n",
              " 'Can',\n",
              " 'Canaan',\n",
              " 'Canada',\n",
              " 'Canadian',\n",
              " 'Canal',\n",
              " 'Canaller',\n",
              " 'Canallers',\n",
              " 'Canals',\n",
              " 'Canaris',\n",
              " 'Cancer',\n",
              " 'Candles',\n",
              " 'Cannibal',\n",
              " 'Cannibals',\n",
              " 'Cannon',\n",
              " 'Canst',\n",
              " 'Cant',\n",
              " 'Canterbury',\n",
              " 'Cap',\n",
              " 'Cape',\n",
              " 'Capes',\n",
              " 'Capricornus',\n",
              " 'Captain',\n",
              " 'Captains',\n",
              " 'Capting',\n",
              " 'Caramba',\n",
              " 'Careful',\n",
              " 'Carefully',\n",
              " 'Carey',\n",
              " 'Carpenter',\n",
              " 'Carpet',\n",
              " 'Carrol',\n",
              " 'Carson',\n",
              " 'Carthage',\n",
              " 'Caryatid',\n",
              " 'Case',\n",
              " 'Cash',\n",
              " 'Cassock',\n",
              " 'Castaway',\n",
              " 'Castle',\n",
              " 'Categut',\n",
              " 'Cathedral',\n",
              " 'Catholic',\n",
              " 'Cato',\n",
              " 'Catskill',\n",
              " 'Cattegat',\n",
              " 'Caught',\n",
              " 'Cave',\n",
              " 'Caw',\n",
              " 'Cellini',\n",
              " 'Central',\n",
              " 'Certain',\n",
              " 'Certainly',\n",
              " 'Cervantes',\n",
              " 'Cetacea',\n",
              " 'Cetacean',\n",
              " 'Cetology',\n",
              " 'Cetus',\n",
              " 'Ceylon',\n",
              " 'Chace',\n",
              " 'Chaldee',\n",
              " 'Champagne',\n",
              " 'Champollion',\n",
              " 'Channel',\n",
              " 'Chapel',\n",
              " 'Charing',\n",
              " 'Charity',\n",
              " 'Charlemagne',\n",
              " 'Charley',\n",
              " 'Chart',\n",
              " 'Chartering',\n",
              " 'Chase',\n",
              " 'Cheever',\n",
              " 'Cherries',\n",
              " 'Chestnut',\n",
              " 'Chief',\n",
              " 'Childe',\n",
              " 'Chili',\n",
              " 'Chilian',\n",
              " 'China',\n",
              " 'Chinese',\n",
              " 'Cholo',\n",
              " 'Chowder',\n",
              " 'Christ',\n",
              " 'Christendom',\n",
              " 'Christian',\n",
              " 'Christianity',\n",
              " 'Christians',\n",
              " 'Christmas',\n",
              " 'Church',\n",
              " 'Cinque',\n",
              " 'Circassian',\n",
              " 'Circumambulate',\n",
              " 'Cistern',\n",
              " 'Civitas',\n",
              " 'Clam',\n",
              " 'Clap',\n",
              " 'Claus',\n",
              " 'Clay',\n",
              " 'Clear',\n",
              " 'Clearing',\n",
              " 'Cleopatra',\n",
              " 'Cleveland',\n",
              " 'Clifford',\n",
              " 'Clinging',\n",
              " 'Clootz',\n",
              " 'Close',\n",
              " 'Closing',\n",
              " 'Cloud',\n",
              " 'Cluny',\n",
              " 'Coast',\n",
              " 'Cock',\n",
              " 'Cockatoo',\n",
              " 'Cod',\n",
              " 'Cods',\n",
              " 'Coenties',\n",
              " 'Coffin',\n",
              " 'Coffins',\n",
              " 'Cognac',\n",
              " 'Coke',\n",
              " 'Cold',\n",
              " 'Coleman',\n",
              " 'Coleridge',\n",
              " 'College',\n",
              " 'Colnett',\n",
              " 'Cologne',\n",
              " 'Colonies',\n",
              " 'Colossus',\n",
              " 'Columbus',\n",
              " 'Come',\n",
              " 'Coming',\n",
              " 'Commanded',\n",
              " 'Commanders',\n",
              " 'Commend',\n",
              " 'Commodore',\n",
              " 'Commodores',\n",
              " 'Common',\n",
              " 'Commonly',\n",
              " 'Commons',\n",
              " 'Commonwealth',\n",
              " 'Companies',\n",
              " 'Comparing',\n",
              " 'Concerning',\n",
              " 'Congo',\n",
              " 'Congregation',\n",
              " 'Congregational',\n",
              " 'Conjuror',\n",
              " 'Connecticut',\n",
              " 'Consequently',\n",
              " 'Consider',\n",
              " 'Considering',\n",
              " 'Constable',\n",
              " 'Constantine',\n",
              " 'Constantinople',\n",
              " 'Consumptive',\n",
              " 'Continents',\n",
              " 'Contrasted',\n",
              " 'Conversation',\n",
              " 'Convulsively',\n",
              " 'Cook',\n",
              " 'Cooke',\n",
              " 'Cooks',\n",
              " 'Cooper',\n",
              " 'Coopman',\n",
              " 'Copenhagen',\n",
              " 'Coppered',\n",
              " 'Corinthians',\n",
              " 'Corkscrew',\n",
              " 'Corlaer',\n",
              " 'Corlears',\n",
              " 'Coronation',\n",
              " 'Corresponding',\n",
              " 'Corrupt',\n",
              " 'Cough',\n",
              " 'Could',\n",
              " 'Count',\n",
              " 'Counterpane',\n",
              " 'County',\n",
              " 'Court',\n",
              " 'Cousin',\n",
              " 'Cowper',\n",
              " 'Crab',\n",
              " 'Crack',\n",
              " 'Crammer',\n",
              " 'Crappo',\n",
              " 'Crappoes',\n",
              " 'Crazed',\n",
              " 'Creagh',\n",
              " 'Created',\n",
              " 'Cretan',\n",
              " 'Crete',\n",
              " 'Crew',\n",
              " 'Crish',\n",
              " 'Crockett',\n",
              " 'Cross',\n",
              " 'Crossed',\n",
              " 'Crossing',\n",
              " 'Crotch',\n",
              " 'Crowding',\n",
              " 'Crown',\n",
              " 'Crozetts',\n",
              " 'Cruelty',\n",
              " 'Cruising',\n",
              " 'Cruppered',\n",
              " 'Crusaders',\n",
              " 'Crushed',\n",
              " 'Crying',\n",
              " 'Cuba',\n",
              " 'Curious',\n",
              " 'Curse',\n",
              " 'Cursed',\n",
              " 'Curses',\n",
              " 'Cussed',\n",
              " 'Customs',\n",
              " 'Cut',\n",
              " 'Cutter',\n",
              " 'Cutting',\n",
              " 'Cuvier',\n",
              " 'Cyclades',\n",
              " 'Czar',\n",
              " 'D',\n",
              " 'Daboll',\n",
              " 'Daggoo',\n",
              " 'Dagon',\n",
              " 'Dame',\n",
              " 'Damn',\n",
              " 'Damocles',\n",
              " 'Dampier',\n",
              " 'Dan',\n",
              " 'Dance',\n",
              " 'Danes',\n",
              " 'Daniel',\n",
              " 'Danish',\n",
              " 'Dante',\n",
              " 'Dantean',\n",
              " 'Dar',\n",
              " 'Dardanelles',\n",
              " 'Darien',\n",
              " 'Darkness',\n",
              " 'Darmonodes',\n",
              " 'Dart',\n",
              " 'Dash',\n",
              " 'Dashing',\n",
              " 'Dauphine',\n",
              " 'Davis',\n",
              " 'Davy',\n",
              " 'Day',\n",
              " 'Days',\n",
              " 'De',\n",
              " 'Deacon',\n",
              " 'Dead',\n",
              " 'Death',\n",
              " 'Decanter',\n",
              " 'Decapitation',\n",
              " 'December',\n",
              " 'Deck',\n",
              " 'Deep',\n",
              " 'Deer',\n",
              " 'Deity',\n",
              " 'Del',\n",
              " 'Deliberately',\n",
              " 'Delight',\n",
              " 'Delightful',\n",
              " 'Deliverer',\n",
              " 'Delta',\n",
              " 'Den',\n",
              " 'Denderah',\n",
              " 'Depend',\n",
              " 'Derick',\n",
              " 'Dericks',\n",
              " 'Descartian',\n",
              " 'Descending',\n",
              " 'Desecrated',\n",
              " 'Desmarest',\n",
              " 'Desolation',\n",
              " 'Despairing',\n",
              " 'Despatch',\n",
              " 'Detached',\n",
              " 'Deuteronomy',\n",
              " 'Devil',\n",
              " 'Devils',\n",
              " 'Dey',\n",
              " 'Diaz',\n",
              " 'Dick',\n",
              " 'Did',\n",
              " 'Didn',\n",
              " 'Didst',\n",
              " 'Diminish',\n",
              " 'Ding',\n",
              " 'Dinner',\n",
              " 'Dinting',\n",
              " 'Discovery',\n",
              " 'Disdain',\n",
              " 'Dish',\n",
              " 'Dismal',\n",
              " 'Dissect',\n",
              " 'Dives',\n",
              " 'Divine',\n",
              " 'Diving',\n",
              " 'Do',\n",
              " 'Doctor',\n",
              " 'Dodge',\n",
              " 'Does',\n",
              " 'Doesn',\n",
              " 'Dog',\n",
              " 'Dolly',\n",
              " 'Dome',\n",
              " 'Dominic',\n",
              " 'Don',\n",
              " 'Dons',\n",
              " 'Doom',\n",
              " 'Dorchester',\n",
              " 'Dost',\n",
              " 'Doubloon',\n",
              " 'Doubtless',\n",
              " 'Doubts',\n",
              " 'Dough',\n",
              " 'Dover',\n",
              " 'Down',\n",
              " 'Dr',\n",
              " 'Dragged',\n",
              " 'Dragon',\n",
              " 'Drat',\n",
              " 'Drawing',\n",
              " 'Drawn',\n",
              " 'Draws',\n",
              " 'Drink',\n",
              " 'Drinking',\n",
              " 'Drive',\n",
              " 'Drop',\n",
              " 'Dropping',\n",
              " 'Dry',\n",
              " 'Duck',\n",
              " 'Dugongs',\n",
              " 'Duke',\n",
              " 'Dunder',\n",
              " 'Dunfermline',\n",
              " 'Dunkirk',\n",
              " 'Duodecimo',\n",
              " 'Duodecimoes',\n",
              " 'Durand',\n",
              " 'Durer',\n",
              " 'During',\n",
              " 'Dusk',\n",
              " 'Dut',\n",
              " 'Dutch',\n",
              " 'Dutchman',\n",
              " 'Dying',\n",
              " 'E',\n",
              " 'Each',\n",
              " 'Eagle',\n",
              " 'Earl',\n",
              " 'Earls',\n",
              " 'Earthsman',\n",
              " 'East',\n",
              " 'Eastern',\n",
              " 'Easy',\n",
              " 'Ebony',\n",
              " 'Ecclesiastes',\n",
              " 'Eckerman',\n",
              " 'Eddystone',\n",
              " 'Edgewise',\n",
              " 'Edmund',\n",
              " 'Edward',\n",
              " 'Ego',\n",
              " 'Egypt',\n",
              " 'Egyptian',\n",
              " 'Egyptians',\n",
              " 'Eh',\n",
              " 'Ehrenbreitstein',\n",
              " 'Eight',\n",
              " 'Either',\n",
              " 'Elbe',\n",
              " 'Electors',\n",
              " 'Elephant',\n",
              " 'Elephanta',\n",
              " 'Elephants',\n",
              " 'Elijah',\n",
              " 'Ellenborough',\n",
              " 'Elsewhere',\n",
              " 'Emblazonings',\n",
              " 'Emboldened',\n",
              " 'Emir',\n",
              " 'Emperor',\n",
              " 'Emperors',\n",
              " 'Empire',\n",
              " 'End',\n",
              " 'Enderbies',\n",
              " 'Enderby',\n",
              " 'Enderbys',\n",
              " 'England',\n",
              " 'Englander',\n",
              " 'English',\n",
              " 'Englishman',\n",
              " 'Englishmen',\n",
              " 'Enough',\n",
              " 'Enter',\n",
              " 'Entering',\n",
              " 'Entreaties',\n",
              " 'Enveloped',\n",
              " 'Ephesian',\n",
              " 'Epilogue',\n",
              " 'Epitome',\n",
              " 'Equality',\n",
              " 'Equator',\n",
              " 'Equatorial',\n",
              " 'Ere',\n",
              " 'Erie',\n",
              " 'Erromanggoans',\n",
              " 'Erroneous',\n",
              " 'Erskine',\n",
              " 'Esau',\n",
              " 'Espied',\n",
              " 'Espying',\n",
              " 'Esquimaux',\n",
              " 'Essex',\n",
              " 'Et',\n",
              " 'Eternities',\n",
              " 'Eternity',\n",
              " 'Ethiopian',\n",
              " 'Euclid',\n",
              " 'Euclidean',\n",
              " 'Euroclydon',\n",
              " 'Europa',\n",
              " 'Europe',\n",
              " 'European',\n",
              " 'Evangelist',\n",
              " 'Evangelists',\n",
              " 'Even',\n",
              " 'Ever',\n",
              " 'Every',\n",
              " 'Evil',\n",
              " 'Ex',\n",
              " 'Excellent',\n",
              " 'Excepting',\n",
              " 'Exception',\n",
              " 'Excuse',\n",
              " 'Expedition',\n",
              " 'Expeditions',\n",
              " 'Explain',\n",
              " 'Exploring',\n",
              " 'Extending',\n",
              " 'Ezekiel',\n",
              " 'F',\n",
              " 'Fa',\n",
              " 'Face',\n",
              " 'Fain',\n",
              " 'Faintly',\n",
              " 'Fair',\n",
              " 'Faith',\n",
              " 'Falsehood',\n",
              " 'Fanning',\n",
              " 'Far',\n",
              " 'Farewell',\n",
              " 'Fashioned',\n",
              " 'Fast',\n",
              " 'Fasting',\n",
              " 'Fat',\n",
              " 'Fata',\n",
              " 'Fate',\n",
              " 'Fates',\n",
              " 'Father',\n",
              " 'Fe',\n",
              " 'Fear',\n",
              " 'Fearing',\n",
              " 'February',\n",
              " 'Fedallah',\n",
              " 'Feegee',\n",
              " 'Feegeeans',\n",
              " 'Feegees',\n",
              " 'Feel',\n",
              " 'Feet',\n",
              " 'Fejee',\n",
              " 'Fellow',\n",
              " 'Ferdinando',\n",
              " 'Fernandes',\n",
              " 'Fetch',\n",
              " 'Few',\n",
              " 'Fields',\n",
              " 'Fiercely',\n",
              " 'Fiery',\n",
              " 'Fife',\n",
              " 'Fifth',\n",
              " 'Figuera',\n",
              " 'Fill',\n",
              " 'Fin',\n",
              " 'Finally',\n",
              " 'Find',\n",
              " 'Finding',\n",
              " 'Fine',\n",
              " 'Fired',\n",
              " 'First',\n",
              " 'Fish',\n",
              " 'Fisheries',\n",
              " 'Fishery',\n",
              " 'Fishes',\n",
              " 'Fishiest',\n",
              " 'Fits',\n",
              " 'Fitz',\n",
              " 'Five',\n",
              " 'Flask',\n",
              " 'Flat',\n",
              " 'Fleece',\n",
              " 'Fleet',\n",
              " 'Flip',\n",
              " 'Floating',\n",
              " 'Floundered',\n",
              " 'Flounders',\n",
              " 'Flukes',\n",
              " 'Flying',\n",
              " 'Fogo',\n",
              " 'Folding',\n",
              " 'Folger',\n",
              " 'Folgers',\n",
              " 'Folio',\n",
              " 'Folios',\n",
              " 'Fool',\n",
              " 'Foolish',\n",
              " 'For',\n",
              " 'Forced',\n",
              " 'Fore',\n",
              " 'Forecastle',\n",
              " 'Forehead',\n",
              " 'Foremost',\n",
              " 'Forge',\n",
              " 'Form',\n",
              " 'Forming',\n",
              " 'Formosa',\n",
              " 'Forthwith',\n",
              " 'Forty',\n",
              " 'Forward',\n",
              " 'Fossil',\n",
              " 'Fountain',\n",
              " 'Fourth',\n",
              " 'France',\n",
              " 'Frankfort',\n",
              " 'Franklin',\n",
              " 'Frederick',\n",
              " 'Free',\n",
              " 'Freely',\n",
              " 'Freeze',\n",
              " 'French',\n",
              " 'Frenchman',\n",
              " 'Frenchmen',\n",
              " 'Friar',\n",
              " 'Friend',\n",
              " 'Friends',\n",
              " 'Friesland',\n",
              " 'Frighted',\n",
              " 'Frobisher',\n",
              " 'Froissart',\n",
              " 'From',\n",
              " 'Fuego',\n",
              " 'Full',\n",
              " 'Funeral',\n",
              " 'Furl',\n",
              " 'Further',\n",
              " 'Furthermore',\n",
              " 'Future',\n",
              " 'Gabriel',\n",
              " 'Gaining',\n",
              " 'Gall',\n",
              " 'Galleries',\n",
              " 'Gallipagos',\n",
              " 'Gam',\n",
              " 'Gamming',\n",
              " 'Ganders',\n",
              " 'Ganges',\n",
              " 'Gardiner',\n",
              " 'Garnery',\n",
              " 'Gases',\n",
              " 'Gate',\n",
              " 'Gather',\n",
              " 'Gay',\n",
              " 'Gayer',\n",
              " 'Gayhead',\n",
              " 'Gazette',\n",
              " 'Gemini',\n",
              " 'General',\n",
              " 'Genesis',\n",
              " 'Geneva',\n",
              " 'Genius',\n",
              " 'Gentlemen',\n",
              " 'Gently',\n",
              " 'Geological',\n",
              " 'George',\n",
              " 'Ger',\n",
              " 'Germain',\n",
              " 'German',\n",
              " 'Germans',\n",
              " 'Gesner',\n",
              " 'Get',\n",
              " 'Ghent',\n",
              " 'Gibraltar',\n",
              " 'Gifted',\n",
              " 'Gilder',\n",
              " 'Ginger',\n",
              " 'Give',\n",
              " 'Giver',\n",
              " 'Giving',\n",
              " 'Glacier',\n",
              " 'Glancing',\n",
              " 'Glen',\n",
              " 'Gliding',\n",
              " 'Glimpses',\n",
              " 'Globe',\n",
              " 'Glory',\n",
              " 'Gnawed',\n",
              " 'Go',\n",
              " 'Goa',\n",
              " 'Goat',\n",
              " 'God',\n",
              " 'Gods',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YWHbsQyIhC4"
      },
      "source": [
        "And there are more. if `wrd` is a string, then, for example:\n",
        "\n",
        "* `wrd.islower()` will return true if the word is all lowercase\n",
        "* `wrd.isalpha()` will return true if all the character in the string are letters\n",
        "\n",
        "and there are also: `wrd.startswith('str')`, `wrd.isdigit()`, `wr.isalnum()`\n",
        "and more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiP7qcUTIUS6"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "### Exercise: \n",
        "\n",
        "def detect_string(tokens: List[str], search_str: str, search_position: int = 0):\n",
        "  \"\"\"Returns a sorted list of the vocabulary tokens which match the search conditions\n",
        "\n",
        "  params:\n",
        "    tokens: a document tokens list.\n",
        "    search_str: a string to search in the token list \n",
        "    search_position: one of the following:\n",
        "      0 - anywhere in the string\n",
        "      1 - searches for the string at the beginning of the token\n",
        "      2 - searches for the string at the end of the token\n",
        "  \"\"\"\n",
        "  ###  Fill in this function to returns the result of searching for the  \n",
        "  ### given string in the vocabulary of the tokens, according to the \n",
        "  ### position parameter\n",
        "  \n",
        "  vocab = set(tokens)\n",
        "  \n",
        "  if search_position == 1:\n",
        "    return sorted([t for t in vocab if t.startswith(search_str)])\n",
        "\n",
        "  if search_position == 2:\n",
        "    return sorted([t for t in vocab if t.endswith(search_str)])\n",
        "\n",
        "  return sorted([t for t in vocab if search_str in t])\n",
        "\n",
        "### "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfyM1irgyT-_",
        "outputId": "241de8a7-c189-4c14-f899-8f03d881e46c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "detect_string(text1, 'larg') "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['enlarge',\n",
              " 'enlarged',\n",
              " 'enlarges',\n",
              " 'large',\n",
              " 'largely',\n",
              " 'largeness',\n",
              " 'larger',\n",
              " 'largest']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_t95CzIL27v"
      },
      "source": [
        "### Test:\n",
        "assert detect_string(text1, 'tably', 2) == ['comfortably',\n",
        " 'discreditably',\n",
        " 'illimitably',\n",
        " 'immutably',\n",
        " 'indubitably',\n",
        " 'inevitably',\n",
        " 'inscrutably',\n",
        " 'profitably',\n",
        " 'unaccountably',\n",
        " 'unwarrantably']"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrBjzeHIL4_V"
      },
      "source": [
        "### Test:\n",
        "assert detect_string(text1, 'argu', 1) == ['argue', 'argued', 'arguing', 'argument', 'arguments']"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOJPKwpnL6nQ"
      },
      "source": [
        "### Test:\n",
        "assert detect_string(text1, 'arg', 2) == []"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8V2lvmWL8TC"
      },
      "source": [
        "### Test\n",
        "assert detect_string(text1, 'larg') == ['enlarge',\n",
        " 'enlarged',\n",
        " 'enlarges',\n",
        " 'large',\n",
        " 'largely',\n",
        " 'largeness',\n",
        " 'larger',\n",
        " 'largest']"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8QYjforL9vI"
      },
      "source": [
        ""
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}